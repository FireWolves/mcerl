{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%env SPDLOG_LEVEL=trace\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "from rl.utils import build_graphs\n",
    "\n",
    "from mcerl.env import Env\n",
    "from mcerl.utils import (\n",
    "    delta_time_reward_standardize,\n",
    "    exploration_reward_rescale,\n",
    "    reward_sum,\n",
    "    single_env_rollout,\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"0.png\")\n",
    "img = ImageOps.grayscale(img)\n",
    "img = img.resize((300, 200))\n",
    "grid_map = np.array(img)\n",
    "plt.imshow(grid_map, cmap=\"gray\",vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "agent_poses = None\n",
    "num_rays = 32\n",
    "max_steps = 100000\n",
    "max_steps_per_agent = 100\n",
    "ray_range = 30\n",
    "velocity = 1\n",
    "min_frontier_size = 8\n",
    "max_frontier_size = 30\n",
    "exploration_threshold = 0.98\n",
    "env = Env(\n",
    "    num_agents=num_agents,\n",
    "    max_steps=max_steps,\n",
    "    max_steps_per_agent=max_steps_per_agent,\n",
    "    velocity=velocity,\n",
    "    sensor_range=ray_range,\n",
    "    num_rays=num_rays,\n",
    "    min_frontier_pixel=min_frontier_size,\n",
    "    max_frontier_pixel=max_frontier_size,\n",
    "    exploration_threshold=exploration_threshold,\n",
    ")\n",
    "# Example usage\n",
    "# num_threads = 15\n",
    "# epochs = 10\n",
    "# rollouts = multi_threaded_rollout(\n",
    "#     env=lambda: Env(\n",
    "#         num_agents=num_agents,\n",
    "#         max_steps=max_steps,\n",
    "#         max_steps_per_agent=max_steps_per_agent,\n",
    "#         velocity=velocity,\n",
    "#         sensor_range=ray_range,\n",
    "#         num_rays=num_rays,\n",
    "#         min_frontier_pixel=min_frontier_size,\n",
    "#         max_frontier_pixel=max_frontier_size,\n",
    "#     ),\n",
    "#     grid_map=grid_map,\n",
    "#     agent_poses=agent_poses,\n",
    "#     policy=policy,\n",
    "#     num_threads=num_threads,\n",
    "#     epochs=epochs,\n",
    "# )\n",
    "# trajectories = []\n",
    "# frame_data = env.reset(grid_map, agent_poses, return_maps=True)\n",
    "# trajectories.append(frame_data)\n",
    "# while True:\n",
    "#     agent_id = frame_data[\"info\"][\"agent_id\"]\n",
    "#     frame_data[\"action_agent_id\"] = agent_id\n",
    "#     frame_data = random_policy(frame_data)\n",
    "#     frame_data = env.step(frame_data, return_maps=True)\n",
    "#     trajectories.append(frame_data)\n",
    "#     if env.done() is True:\n",
    "#         break\n",
    "# rollouts = split_trajectories(trajectories)\n",
    "# rollouts = [pad_trajectory(rollout) for rollout in rollouts]\n",
    "# rollouts = [refine_trajectory(rollout) for rollout in rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.actor_critic import GINPolicyNetwork\n",
    "from torch.nn import functional as F\n",
    "\n",
    "network = GINPolicyNetwork(dim_h=32)\n",
    "\n",
    "pred = network(\n",
    "    rollout[0][\"observation\"][\"graph\"].x,\n",
    "    rollout[0][\"observation\"][\"graph\"].edge_index,\n",
    "    rollout[0][\"observation\"][\"graph\"].batch,\n",
    ")\n",
    "probabilities = F.softmax(pred, dim=0)\n",
    "action_index = torch.multinomial(probabilities.squeeze(), 1)\n",
    "log_prob = torch.log(probabilities[action_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts = single_env_rollout(env, grid_map,policy=network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [\n",
    "    Image.fromarray(frame_data[\"observation\"][\"global_map\"])\n",
    "    for frame_data in rollouts[0]\n",
    "]\n",
    "imgs[0].save(\"array0.gif\", save_all=True, append_images=imgs[1:], duration=50, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_step_exploration_reward = ray_range * 2 * 1.41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts = [build_graphs(rollout) for rollout in rollouts]\n",
    "rollouts = [\n",
    "    exploration_reward_rescale(rollout, max_value=max_step_exploration_reward)\n",
    "    for rollout in rollouts\n",
    "]\n",
    "rollouts = [delta_time_reward_standardize(rollout) for rollout in rollouts]\n",
    "rollouts = [reward_sum(rollout) for rollout in rollouts]\n",
    "rollouts = [build_graphs(rollout) for rollout in rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = rollouts[2]\n",
    "[frame_data[\"reward_to_go\"] for frame_data in rollout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
