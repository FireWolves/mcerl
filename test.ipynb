{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "from math import pi\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "from rl.actor_critic import GAE, Actor, ActorCritic, Critic\n",
    "from rl.network import GINPolicyNetwork, GINValueNetwork\n",
    "from rl.utils import Sampler, to_graph\n",
    "from torch.optim.adam import Adam\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from mcerl.env import Env\n",
    "from mcerl.utils import (\n",
    "    delta_time_reward_standardize,\n",
    "    exploration_reward_rescale,\n",
    "    multi_threaded_rollout,\n",
    "    reward_sum,\n",
    "    single_env_rollout,  # noqa: F401\n",
    ")\n",
    "\n",
    "%env SPDLOG_LEVEL=warning\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load map\n",
    "img = Image.open(\"0.png\")\n",
    "img = ImageOps.grayscale(img)\n",
    "img = img.resize((300, 200))\n",
    "grid_map = np.array(img)\n",
    "grid_map[grid_map < 100] = 0\n",
    "grid_map[grid_map >= 100] = 255\n",
    "plt.imshow(grid_map, cmap=\"gray\", vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "#########################\n",
    "# 环境参数\n",
    "#########################\n",
    "\n",
    "\n",
    "exclude_parameters = list(locals().keys())\n",
    "# 几个agent\n",
    "num_agents = 3\n",
    "\n",
    "# agent的初始位置\n",
    "agent_poses = None\n",
    "\n",
    "# agent的初始方向\n",
    "num_rays = 32\n",
    "\n",
    "# 一个env最多迭代多少步\n",
    "max_steps = 100000\n",
    "\n",
    "# 一个agent最多迭代多少步\n",
    "max_steps_per_agent = 100\n",
    "\n",
    "# 传感器的范围\n",
    "ray_range = 30\n",
    "\n",
    "# 速度(pixel per step)\n",
    "velocity = 1\n",
    "\n",
    "# 最小的frontier pixel数\n",
    "min_frontier_size = 8\n",
    "\n",
    "# 最大的frontier pixel数\n",
    "max_frontier_size = 30\n",
    "\n",
    "# 探索的阈值\n",
    "exploration_threshold = 0.98\n",
    "\n",
    "# 地图的高度和宽度\n",
    "map_height, map_width = grid_map.shape\n",
    "\n",
    "# 一个frontier最多可以获得多少信息增益\n",
    "max_exploration_gain = ray_range**2 * pi / 2.0\n",
    "\n",
    "\n",
    "#########################\n",
    "# PPO参数\n",
    "#########################\n",
    "\n",
    "# gae权重\n",
    "lmbda = 0.95\n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# clip范围\n",
    "clip_coefficient = 0.2\n",
    "\n",
    "# 最大gradient范数\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "#########################\n",
    "# 训练参数\n",
    "#########################\n",
    "\n",
    "# 设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 总共训练的次数\n",
    "n_iters = 1000\n",
    "\n",
    "# 每次训练的frame数\n",
    "n_frames_per_iter = 10000\n",
    "\n",
    "# 每次训练的并行环境数(num worker)\n",
    "n_parallel_envs = 15\n",
    "\n",
    "# 每次训练的epoch数, 即数据要被训练多少次\n",
    "n_epochs_per_iter = 5\n",
    "\n",
    "# 每次epoch的mini_batch大小\n",
    "mini_batch_size = n_frames_per_iter // 10\n",
    "\n",
    "# 每个agent的最大步数\n",
    "max_steps_per_agent = 50\n",
    "\n",
    "# 每次训练所用到的总的环境数量\n",
    "n_envs_per_iter = round(n_frames_per_iter / num_agents / max_steps_per_agent)\n",
    "\n",
    "# 每个epoch的frame数\n",
    "n_frames_per_epoch = n_frames_per_iter * n_epochs_per_iter\n",
    "\n",
    "# 每个epoch的mini_batch数\n",
    "n_minibatches_per_epoch = n_frames_per_epoch // mini_batch_size\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    k: v\n",
    "    for k, v in locals().items()\n",
    "    if k not in [*exclude_parameters, \"exclude_parameters\"]\n",
    "}\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment\n",
    "env = Env(\n",
    "    num_agents=num_agents,\n",
    "    max_steps=max_steps,\n",
    "    max_steps_per_agent=max_steps_per_agent,\n",
    "    velocity=velocity,\n",
    "    sensor_range=ray_range,\n",
    "    num_rays=num_rays,\n",
    "    min_frontier_pixel=min_frontier_size,\n",
    "    max_frontier_pixel=max_frontier_size,\n",
    "    exploration_threshold=exploration_threshold,\n",
    ")\n",
    "\n",
    "def env_transform(frame_data: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    normalize position, exploration gain,etc.\n",
    "    \"\"\"\n",
    "    # normalize frontier position to [0,1] and exploration gain to [0,1]\n",
    "    width = float(map_width)\n",
    "    height = float(map_height)\n",
    "    frame_data[\"observation\"][\"frontier_points\"] = [\n",
    "        (\n",
    "            float(x) / width,\n",
    "            float(y) / height,\n",
    "            float(gain) / max_exploration_gain,\n",
    "        )\n",
    "        for x, y, gain in frame_data[\"observation\"][\"frontier_points\"]\n",
    "    ]\n",
    "    # normalize position to [0,1]\n",
    "    frame_data[\"observation\"][\"pos\"] = [\n",
    "        (float(x) / width, float(y) / height)\n",
    "        for x, y in frame_data[\"observation\"][\"pos\"]\n",
    "    ]\n",
    "    frame_data[\"observation\"][\"target_pos\"] = [\n",
    "        (float(x) / width, float(y) / height)\n",
    "        for x, y in frame_data[\"observation\"][\"target_pos\"]\n",
    "    ]\n",
    "    # build graph\n",
    "    frame_data = to_graph(frame_data)\n",
    "    return frame_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define policy\n",
    "def device_cast(\n",
    "    frame_data: dict[str, Any], device: torch.device = torch.device(\"cuda\")\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    cast data to device\n",
    "    \"\"\"\n",
    "    frame_data[\"observation\"][\"graph\"] = frame_data[\"observation\"][\"graph\"].to(device)\n",
    "\n",
    "    return frame_data\n",
    "\n",
    "\n",
    "policy_network = GINPolicyNetwork(dim_h=32)\n",
    "value_network = GINValueNetwork(dim_h=32)\n",
    "actor = Actor(policy_network=policy_network)\n",
    "critic = Critic(value_network=value_network)\n",
    "wrapped_actor_critic = ActorCritic(\n",
    "    actor=actor, critic=critic, forward_preprocess=device_cast\n",
    ")\n",
    "wrapped_actor_critic = wrapped_actor_critic.to(device)\n",
    "value_estimator = GAE(gamma=gamma, lmbda=lmbda)\n",
    "optimizer = Adam(wrapped_actor_critic.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "# single-threaded rollout\n",
    "\n",
    "# single_rollouts = single_env_rollout(env, grid_map, policy=wrapped_actor_critic,env_transform=env_transform)\n",
    "# rollouts=single_rollouts\n",
    "\n",
    "# multi-threaded rollout\n",
    "# num_threads = 20\n",
    "# epochs = 100\n",
    "# rollouts = multi_threaded_rollout(\n",
    "#     env=lambda: Env(\n",
    "#         num_agents=num_agents,\n",
    "#         max_steps=max_steps,\n",
    "#         max_steps_per_agent=max_steps_per_agent,\n",
    "#         velocity=velocity,\n",
    "#         sensor_range=ray_range,\n",
    "#         num_rays=num_rays,\n",
    "#         min_frontier_pixel=min_frontier_size,\n",
    "#         max_frontier_pixel=max_frontier_size,\n",
    "#     ),\n",
    "#     grid_map=grid_map,\n",
    "#     agent_poses=agent_poses,\n",
    "#     policy=actor,\n",
    "#     num_threads=num_threads,\n",
    "#     epochs=epochs,\n",
    "#     return_maps=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for logging\n",
    "timestamp_str = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "writer = SummaryWriter(f\"runs/ppo{timestamp_str}\")\n",
    "writer.add_text(\"parameters\", str(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "for _n_iter in tqdm.tqdm(range(n_iters)):\n",
    "    # collect data\n",
    "    iter_start_time = time.time()\n",
    "    data_collection_start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        ##################\n",
    "        # BEGIN DEBUGGING\n",
    "        ##################\n",
    "        # rollouts = single_env_rollout(env, grid_map, policy=wrapped_actor_critic,env_transform=env_transform)\n",
    "        ##################\n",
    "        # END DEBUGGING\n",
    "        ##################\n",
    "        env_rollout_start_time = time.time()\n",
    "        rollouts = multi_threaded_rollout(\n",
    "            env=lambda: Env(\n",
    "                num_agents=num_agents,\n",
    "                max_steps=max_steps,\n",
    "                max_steps_per_agent=max_steps_per_agent,\n",
    "                velocity=velocity,\n",
    "                sensor_range=ray_range,\n",
    "                num_rays=num_rays,\n",
    "                min_frontier_pixel=min_frontier_size,\n",
    "                max_frontier_pixel=max_frontier_size,\n",
    "            ),\n",
    "            grid_map=grid_map,\n",
    "            agent_poses=agent_poses,\n",
    "            policy=wrapped_actor_critic,\n",
    "            num_threads=n_parallel_envs,\n",
    "            epochs=n_envs_per_iter,\n",
    "            return_maps=False,\n",
    "            env_transform=env_transform,\n",
    "        )\n",
    "        env_rollout_end_time = time.time()\n",
    "        # print(f\"env rollout time: {env_rollout_end_time - env_rollout_start_time}\")\n",
    "\n",
    "        data_post_process_start_time = time.time()\n",
    "        # normalize rewards\n",
    "        rollouts = [\n",
    "            exploration_reward_rescale(rollout, max_value=max_exploration_gain)\n",
    "            for rollout in rollouts\n",
    "        ]\n",
    "        rollouts = [delta_time_reward_standardize(rollout) for rollout in rollouts]\n",
    "        rollouts = [reward_sum(rollout, gamma=gamma) for rollout in rollouts]\n",
    "        data_post_process_end_time = time.time()\n",
    "        # print(\n",
    "        # f\"data post process time: {data_post_process_end_time - data_post_process_start_time}\"\n",
    "        # )\n",
    "\n",
    "        gae_start_time = time.time()\n",
    "        # compute GAE\n",
    "        rollouts = [value_estimator(rollout) for rollout in rollouts]\n",
    "        flattened_rollouts = [\n",
    "            frame_data for rollout in rollouts for frame_data in rollout\n",
    "        ]\n",
    "        gae_end_time = time.time()\n",
    "        # print(f\"GAE time: {gae_end_time - gae_start_time}\")\n",
    "        # add to sampler\n",
    "\n",
    "        # frame_indices = None\n",
    "        graphs = []\n",
    "        rewards = []\n",
    "        values = []\n",
    "        advantages = []\n",
    "        log_probs = []\n",
    "        returns = []\n",
    "        total_times = []\n",
    "\n",
    "        for frame_data in flattened_rollouts:\n",
    "            graphs.append(frame_data[\"observation\"][\"graph\"])\n",
    "            rewards.append(frame_data[\"next\"][\"reward\"][\"total_reward\"])\n",
    "            values.append(frame_data[\"value\"])\n",
    "            advantages.append(frame_data[\"advantage\"])\n",
    "            log_probs.append(frame_data[\"log_prob\"])\n",
    "            returns.append(frame_data[\"return\"])\n",
    "            total_times.append(frame_data[\"info\"][\"total_time_step\"])\n",
    "            # if frame_indices is None:\n",
    "            #     frame_indices = torch.zeros(graph.num_graphs)\n",
    "            # else:\n",
    "            #     frame_indices = torch.cat(\n",
    "            #         [\n",
    "            #             frame_indices,\n",
    "            #             torch.ones(graph.num_graphs) * (frame_indices.max() + 1),\n",
    "            #         ]\n",
    "            #     )\n",
    "        #     for i in range(graph.num_graphs):\n",
    "        #         graphs.append(graph[i])\n",
    "\n",
    "        # graphs = DataLoader(\n",
    "        #     graphs,\n",
    "        #     batch_size=len(graphs),\n",
    "        # )\n",
    "        # graphs = next(iter(graphs))\n",
    "        # graphs = graphs.to(device)\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        values = torch.tensor(values).to(device)\n",
    "        advantages = torch.tensor(advantages).to(device)\n",
    "        log_probs = torch.tensor(log_probs).to(device)\n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        total_times = torch.tensor(total_times).to(device)\n",
    "\n",
    "        sampler = Sampler(\n",
    "            batch_size=mini_batch_size,\n",
    "            length=len(flattened_rollouts),\n",
    "            graphs=graphs,\n",
    "            rewards=rewards,\n",
    "            values=values,\n",
    "            advantages=advantages,\n",
    "            log_probs=log_probs,\n",
    "            returns=returns,\n",
    "            total_times=total_times,\n",
    "        )\n",
    "    data_collection_end_time = time.time()\n",
    "    # print(\n",
    "    # f\"data collection time: {data_collection_end_time - data_collection_start_time}\"\n",
    "    # )\n",
    "    ##################\n",
    "    # BEGIN DEBUGGING\n",
    "    ##################\n",
    "    # sampler = Sampler(\n",
    "    #     rollout=[frame_data for rollout in rollouts for frame_data in rollout],\n",
    "    #     batch_size=10,\n",
    "    # )\n",
    "    ##################\n",
    "    # END DEBUGGING\n",
    "    ##################\n",
    "    # epoch loop\n",
    "    clip_fraction = []\n",
    "    ##################\n",
    "    # BEGIN DEBUGGING\n",
    "    ##################\n",
    "    # for _n_epoch in range(1):\n",
    "    ##################\n",
    "    # END DEBUGGING\n",
    "    ##################\n",
    "    training_start_time = time.time()\n",
    "    for _n_epoch in range(n_epochs_per_iter):\n",
    "        epoch_start_time = time.time()\n",
    "        ##################\n",
    "        # BEGIN DEBUGGING\n",
    "        ##################\n",
    "        # for _n_mini_batch in range(1):\n",
    "        ##################\n",
    "        # END DEBUGGING\n",
    "        ##################\n",
    "        for _n_mini_batch in range(n_frames_per_iter // mini_batch_size):\n",
    "            mini_batch_start_time = time.time()\n",
    "\n",
    "            sample_start_time = time.time()\n",
    "            # sample data\n",
    "            mini_batch_data = sampler.random_sample()\n",
    "            graphs = mini_batch_data[\"graphs\"].to(device)\n",
    "            rewards = mini_batch_data[\"rewards\"]\n",
    "            values = mini_batch_data[\"values\"].to(device).flatten()\n",
    "            advantages = mini_batch_data[\"advantages\"].to(device).flatten()\n",
    "            prev_log_prob = mini_batch_data[\"log_probs\"].to(device).flatten()\n",
    "            returns = mini_batch_data[\"returns\"].to(device).flatten()\n",
    "            total_times = mini_batch_data[\"total_times\"]\n",
    "            frame_indices = mini_batch_data[\"frame_indices\"].to(device)\n",
    "            sample_end_time = time.time()\n",
    "            # print(f\"sample time: {sample_end_time - sample_start_time}\")\n",
    "\n",
    "            training_data_prepare_start_time = time.time()\n",
    "            # get minibatch data and transform to tensor for training\n",
    "\n",
    "            forward_start_time = time.time()\n",
    "            _, new_log_probs, new_values = wrapped_actor_critic.forward_parallel(\n",
    "                graphs, frame_indices\n",
    "            )\n",
    "            new_log_probs = new_log_probs.to(device).flatten()\n",
    "            new_values = new_values.to(device).flatten()\n",
    "            forward_end_time = time.time()\n",
    "            # print(f\"forward time: {forward_end_time - forward_start_time}\")\n",
    "\n",
    "            training_data_prepare_end_time = time.time()\n",
    "            # print(\n",
    "            # f\"training data prepare time: {training_data_prepare_end_time - training_data_prepare_start_time}\"\n",
    "            # )\n",
    "            loss_compute_start_time = time.time()\n",
    "            # compute loss\n",
    "            log_ratio = new_log_probs - prev_log_prob\n",
    "            ratio = log_ratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                old_approx_kl = (-log_ratio).mean()\n",
    "                approx_kl = ((ratio - 1) - log_ratio).mean()\n",
    "                clip_fraction += [\n",
    "                    ((ratio - 1.0).abs() > clip_coefficient).float().mean().item()\n",
    "                ]\n",
    "\n",
    "            pg_loss1 = -advantages * ratio\n",
    "            pg_loss2 = -advantages * torch.clamp(\n",
    "                ratio, 1 - clip_coefficient, 1 + clip_coefficient\n",
    "            )\n",
    "\n",
    "            policy_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "            value_loss = 0.5 * ((new_values - returns) ** 2).mean()\n",
    "            loss = policy_loss + value_loss\n",
    "            loss_compute_end_time = time.time()\n",
    "            # print(\n",
    "            # f\"loss compute time: {loss_compute_end_time - loss_compute_start_time}\"\n",
    "            # )\n",
    "\n",
    "            optimizer_start_time = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                wrapped_actor_critic.parameters(), max_grad_norm\n",
    "            )\n",
    "            optimizer.step()\n",
    "            optimizer_end_time = time.time()\n",
    "            # print(f\"optimizer time: {optimizer_end_time - optimizer_start_time}\")\n",
    "            mini_batch_end_time = time.time()\n",
    "            # print(f\"mini batch time: {mini_batch_end_time - mini_batch_start_time}\")\n",
    "        epoch_end_time = time.time()\n",
    "        # print(f\"epoch time: {epoch_end_time - epoch_start_time}\")\n",
    "    training_end_time = time.time()\n",
    "    # print(f\"training time: {training_end_time - training_start_time}\")\n",
    "\n",
    "    average_epsiode_time = torch.mean(total_times.to(torch.float)).item()\n",
    "    episode_reward_mean = torch.mean(returns).item()\n",
    "\n",
    "    global_step = _n_iter\n",
    "    if global_step % 10 == 0 and global_step > 0:\n",
    "        torch.save(wrapped_actor_critic.state_dict(), f\"ppo_parallel_{global_step}.pt\")\n",
    "    writer.add_scalar(\"losses/value_loss\", value_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", policy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/clip_fraction\", np.mean(clip_fraction), global_step)\n",
    "    writer.add_scalar(\"info/episode_reward_mean\", episode_reward_mean, global_step)\n",
    "    writer.add_scalar(\"info/average_epsiode_time\", average_epsiode_time, global_step)\n",
    "    iter_end_time = time.time()\n",
    "    # print(f\"iter time: {iter_end_time - iter_start_time}\")\n",
    "\n",
    "torch.save(wrapped_actor_critic.state_dict(), f\"ppo_parallel_{global_step}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw raw rewards for debugging\n",
    "\n",
    "#print(\"exploration reward:\")\n",
    "for rollout in rollouts:\n",
    "    plt.plot(\n",
    "        [frame_data[\"next\"][\"reward\"][\"exploration_reward\"] for frame_data in rollout]\n",
    "    )\n",
    "plt.show()\n",
    "#print(\"time_step_reward :\")\n",
    "for rollout in rollouts:\n",
    "    plt.plot(\n",
    "        [frame_data[\"next\"][\"reward\"][\"time_step_reward\"] for frame_data in rollout]\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbda = 0.95\n",
    "gamma = 0.99\n",
    "# normalize rewards\n",
    "rollouts = [\n",
    "    exploration_reward_rescale(rollout, max_value=max_exploration_gain)\n",
    "    for rollout in rollouts\n",
    "]\n",
    "rollouts = [delta_time_reward_standardize(rollout) for rollout in rollouts]\n",
    "rollouts = [reward_sum(rollout,gamma=gamma) for rollout in rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute GAE\n",
    "from rl.actor_critic import GAE\n",
    "\n",
    "gae = GAE(gamma=gamma, lmbda=lmbda)\n",
    "rollouts = [gae(rollout) for rollout in rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw normalized rewards for debugging\n",
    "# total_reward\n",
    "for rollout in rollouts:\n",
    "    plt.plot([frame_data[\"next\"][\"reward\"][\"total_reward\"] for frame_data in rollout])\n",
    "mean_val = np.mean(\n",
    "    [\n",
    "        frame_data[\"next\"][\"reward\"][\"total_reward\"]\n",
    "        for frame_data in rollout\n",
    "        for rollout in rollouts\n",
    "    ]\n",
    ")\n",
    "plt.title(f\"total reward:{mean_val}\")\n",
    "plt.axhline(mean_val, color=\"r\", linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# exploration_reward\n",
    "for rollout in rollouts:\n",
    "    plt.plot(\n",
    "        [frame_data[\"next\"][\"reward\"][\"exploration_reward\"] for frame_data in rollout]\n",
    "    )\n",
    "mean_val = np.mean(\n",
    "    [\n",
    "        frame_data[\"next\"][\"reward\"][\"exploration_reward\"]\n",
    "        for frame_data in rollout\n",
    "        for rollout in rollouts\n",
    "    ]\n",
    ")\n",
    "plt.title(f\"exploration reward:{mean_val}\")\n",
    "plt.axhline(mean_val, color=\"r\", linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "# time_step_reward\n",
    "for rollout in rollouts:\n",
    "    plt.plot(\n",
    "        [frame_data[\"next\"][\"reward\"][\"time_step_reward\"] for frame_data in rollout]\n",
    "    )\n",
    "mean_val = np.mean(\n",
    "    [\n",
    "        frame_data[\"next\"][\"reward\"][\"time_step_reward\"]\n",
    "        for frame_data in rollout\n",
    "        for rollout in rollouts\n",
    "    ]\n",
    ")\n",
    "plt.title(f\"time step reward:{mean_val}\")\n",
    "plt.axhline(mean_val, color=\"r\", linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "# reward_to_go\n",
    "for rollout in rollouts:\n",
    "    plt.plot([frame_data[\"reward_to_go\"] for frame_data in rollout])\n",
    "mean_val = np.mean(\n",
    "    [frame_data[\"reward_to_go\"] for frame_data in rollout for rollout in rollouts]\n",
    ")\n",
    "plt.title(f\"reward to go:{mean_val}\")\n",
    "plt.axhline(mean_val, color=\"r\", linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# reward_mean\n",
    "reward_mean = [\n",
    "    np.mean([frame_data[\"reward_to_go\"] for frame_data in rollout])\n",
    "    for rollout in rollouts\n",
    "]\n",
    "plt.plot(reward_mean)\n",
    "plt.title(f\"reward mean:{np.mean(reward_mean)}\")\n",
    "plt.axhline(y=np.mean(reward_mean), color=\"r\", linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "# gae\n",
    "gaes = [frame_data[\"advantage\"].item() for frame_data in rollout for rollout in rollouts]\n",
    "plt.plot(gaes)\n",
    "plt.title(f\"gae:{np.mean(gaes)}\")\n",
    "plt.axhline(y=np.mean(gaes), color=\"r\", linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts = single_env_rollout(env, grid_map, policy=wrapped_actor_critic,env_transform=env_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store maps to gif\n",
    "for rollout in rollouts:\n",
    "    agent_id = rollout[0][\"info\"][\"agent_id\"]\n",
    "    imgs = [\n",
    "        Image.fromarray(frame_data[\"observation\"][\"agent_map\"])\n",
    "        for frame_data in rollout\n",
    "    ]\n",
    "    imgs[0].save(\n",
    "        f\"agent_{agent_id}.gif\",\n",
    "        save_all=True,\n",
    "        append_images=imgs[1:],\n",
    "        duration=100,\n",
    "        loop=0,\n",
    "    )\n",
    "flattened_rollouts = []\n",
    "for rollout in rollouts:\n",
    "    flattened_rollouts.extend(rollout)\n",
    "sorted_rollouts = sorted(flattened_rollouts, key=lambda x: x[\"info\"][\"step_cnt\"])\n",
    "imgs = [\n",
    "    Image.fromarray(frame_data[\"observation\"][\"global_map\"])\n",
    "    for frame_data in sorted_rollouts\n",
    "]\n",
    "imgs[0].save(\n",
    "    \"global_map.gif\",\n",
    "    save_all=True,\n",
    "    append_images=imgs[1:],\n",
    "    duration=100,\n",
    "    loop=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dict_struct(d: dict[str, Any], prefix: str = \"\") -> list:\n",
    "    flattened_keys = []\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            flattened_keys.extend(check_dict_struct(v, prefix + k + \".\"))\n",
    "        else:\n",
    "            flattened_keys.append(prefix + k)\n",
    "    return flattened_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "graphs = []\n",
    "for rollout in rollouts:\n",
    "    for frame_data in rollout:\n",
    "        graphs.append(frame_data[\"observation\"][\"graph\"])\n",
    "indices = [1, 3, 5, 6]\n",
    "graphs = [graphs[i] for i in indices]\n",
    "# #print(graphs)\n",
    "frame_indices = None\n",
    "sampled_graphs = []\n",
    "for graph in graphs:\n",
    "    #print(\"one frame has \", graph.num_graphs, \" graphs\")\n",
    "    if frame_indices is None:\n",
    "        frame_indices = torch.zeros(graph.num_graphs)\n",
    "    else:\n",
    "        frame_indices = torch.cat(\n",
    "            [\n",
    "                frame_indices,\n",
    "                torch.ones(graph.num_graphs) * (frame_indices.max() + 1),\n",
    "            ]\n",
    "        )\n",
    "    for i in range(graph.num_graphs):\n",
    "        sampled_graphs.append(graph[i])\n",
    "sampled_graphs = DataLoader(\n",
    "    sampled_graphs,\n",
    "    batch_size=len(sampled_graphs),\n",
    ")\n",
    "sampled_graphs = next(iter(sampled_graphs))\n",
    "sampled_graphs, frame_indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
