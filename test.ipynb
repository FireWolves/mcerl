{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import annotations\n",
    "\n",
    "import concurrent.futures\n",
    "import datetime\n",
    "import logging\n",
    "import pathlib as pl\n",
    "import random\n",
    "import shutil\n",
    "import string\n",
    "import time\n",
    "from math import pi\n",
    "from typing import Any\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rl.actor_critic import GAE, Actor, ActorCritic, Critic\n",
    "from rl.network import GINPolicyNetwork, GINValueNetwork\n",
    "from rl.utils import Sampler, to_graph\n",
    "from torch.optim.adam import Adam\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from mcerl.env import Env\n",
    "from mcerl.utils import (\n",
    "    delta_time_reward_standardize,\n",
    "    exploration_reward_rescale,\n",
    "    reward_sum,\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load map\n",
    "maps = []\n",
    "easy_maps = [\n",
    "    \"map/easy/12.png\",\n",
    "    \"map/easy/95.png\",\n",
    "    \"map/easy/83.png\",\n",
    "    \"map/easy/69.png\",\n",
    "    \"map/easy/50.png\",\n",
    "]\n",
    "hard_maps = [\"map/hard/53.png\", \"map/40.png\"]\n",
    "medium_maps = [\"map/medium/12.png\", \"map/40.png\", \"map/medium/11.png\"]\n",
    "\n",
    "map_height, map_width = 300, 200\n",
    "\n",
    "for map_img in easy_maps + hard_maps + medium_maps:\n",
    "    img = Image.open(map_img)\n",
    "    img = ImageOps.grayscale(img)\n",
    "    img = img.resize((map_width, map_height))\n",
    "    grid_map = np.array(img)\n",
    "    grid_map[grid_map < 100] = 0\n",
    "    grid_map[grid_map >= 100] = 255\n",
    "    maps.append(grid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "#########################\n",
    "# log参数\n",
    "#########################\n",
    "\n",
    "# workspace\n",
    "workspace_dir = pl.Path.cwd()\n",
    "# output\n",
    "output_dir = pl.Path(workspace_dir) / \"output\"\n",
    "current_date = datetime.datetime.now().strftime(\"%Y_%m_%d_%H-%M-%S\")\n",
    "random_string = \"\".join(random.choices(string.ascii_letters + string.digits, k=6))\n",
    "session_name = f\"{current_date}_{random_string}\"\n",
    "# experiment\n",
    "experiment_dir = output_dir / session_name\n",
    "log_dir = experiment_dir / \"log\"\n",
    "cpp_env_log_path = log_dir / \"env.log\"\n",
    "py_env_log_path = log_dir / \"env_py.log\"\n",
    "model_dir = experiment_dir / \"model\"\n",
    "output_images_dir = experiment_dir / \"images\"\n",
    "tensorboard_dir = experiment_dir / \"tensorboard\"\n",
    "\n",
    "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "tensorboard_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# logger\n",
    "log_level = \"warning\"\n",
    "cpp_env_log_level = log_level\n",
    "py_env_log_level = log_level\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(py_env_log_level.upper())\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "logger.addHandler(logging.FileHandler(py_env_log_path, mode=\"w\"))\n",
    "\n",
    "#########################\n",
    "# 环境参数\n",
    "#########################\n",
    "\n",
    "\n",
    "exclude_parameters = list(locals().keys())\n",
    "\n",
    "# 几个agent\n",
    "num_agents = 3\n",
    "\n",
    "# agent的初始位置\n",
    "agent_poses = None\n",
    "\n",
    "# agent的初始方向\n",
    "num_rays = 16\n",
    "\n",
    "# 一个env最多迭代多少步\n",
    "max_steps = 100000\n",
    "\n",
    "# 一个agent最多迭代多少步\n",
    "max_steps_per_agent = 100\n",
    "\n",
    "# 传感器的范围\n",
    "ray_range = 30\n",
    "\n",
    "# 速度(pixel per step)\n",
    "velocity = 1\n",
    "\n",
    "# 最小的frontier pixel数\n",
    "min_frontier_size = 8\n",
    "\n",
    "# 最大的frontier pixel数\n",
    "max_frontier_size = 30\n",
    "\n",
    "# 探索的阈值\n",
    "exploration_threshold = 0.95\n",
    "\n",
    "\n",
    "# 一个frontier最多可以获得多少信息增益\n",
    "max_exploration_gain = ray_range**2 * pi / 2.0\n",
    "\n",
    "\n",
    "#########################\n",
    "# PPO参数\n",
    "#########################\n",
    "\n",
    "# gae权重\n",
    "lmbda = 0.98\n",
    "\n",
    "# discount factor\n",
    "gamma = 0.96\n",
    "\n",
    "# reward scale\n",
    "# 0.5*tanh((x-average_val)/stddev_val)+0.5\n",
    "average_val = 852\n",
    "stddev_val = 765\n",
    "# 1/(1+((x-x_star)/sigma)^(2*b))\n",
    "b = 1\n",
    "x_star = 10\n",
    "sigma = 40\n",
    "\n",
    "# clip范围\n",
    "clip_coefficient = 0.2\n",
    "\n",
    "# 最大gradient范数\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# ESS\n",
    "entropy_coefficient = 0.03\n",
    "\n",
    "entropy_coefficient_decay = 0.995\n",
    "\n",
    "# policy loss的权重\n",
    "policy_loss_weight = 1.0\n",
    "\n",
    "# value loss的权重\n",
    "value_loss_weight = 1.0\n",
    "\n",
    "# learning rate\n",
    "lr = 5e-4\n",
    "#########################\n",
    "# 训练参数\n",
    "#########################\n",
    "\n",
    "# 设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 总共训练的次数\n",
    "n_iters = 1000\n",
    "\n",
    "# 每次训练的frame数(大约)\n",
    "n_frames_per_iter = 5000\n",
    "\n",
    "# 每次训练的并行环境数(num worker)\n",
    "n_parallel_envs = 31\n",
    "\n",
    "# 每次训练的epoch数, 即数据要被训练多少次\n",
    "n_epochs_per_iter = 8\n",
    "\n",
    "# 每次epoch的mini_batch大小\n",
    "n_frames_per_mini_batch = 512\n",
    "\n",
    "# 每个agent的最大步数\n",
    "max_steps_per_agent = 40\n",
    "\n",
    "# 每次训练所用到的总的环境数量\n",
    "# n_envs = round(n_frames_per_iter / num_agents / max_steps_per_agent)\n",
    "\n",
    "n_envs = 100\n",
    "\n",
    "# 每个epoch的frame数\n",
    "n_frames_per_epoch = n_frames_per_iter\n",
    "\n",
    "# 每个epoch的mini_batch数\n",
    "n_mini_batches_per_epoch = n_frames_per_epoch // n_frames_per_mini_batch\n",
    "\n",
    "\n",
    "parameters = {k: v for k, v in locals().items() if k not in [*exclude_parameters, \"exclude_parameters\"]}\n",
    "console = Console()\n",
    "console.print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform function\n",
    "# env_transform\n",
    "def env_transform(frame_data: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    normalize position, exploration gain,etc.\n",
    "    Note that we need to check if env is done\n",
    "    \"\"\"\n",
    "\n",
    "    width = float(map_width)\n",
    "    height = float(map_height)\n",
    "\n",
    "    # normalize frontier position to [0,1] and exploration gain to [0,1]\n",
    "    frame_data[\"observation\"][\"frontier_points\"] = [\n",
    "        (\n",
    "            float(x) / width,\n",
    "            float(y) / height,\n",
    "            float(gain) / max_exploration_gain,\n",
    "            float(distance) / height,\n",
    "        )\n",
    "        for x, y, gain, distance in frame_data[\"observation\"][\"frontier_points\"]\n",
    "    ]\n",
    "\n",
    "    # normalize position to [0,1]\n",
    "    frame_data[\"observation\"][\"pos\"] = [\n",
    "        (float(x) / width, float(y) / height) for x, y in frame_data[\"observation\"][\"pos\"]\n",
    "    ]\n",
    "\n",
    "    frame_data[\"observation\"][\"target_pos\"] = [\n",
    "        (float(x) / width, float(y) / height) for x, y in frame_data[\"observation\"][\"target_pos\"]\n",
    "    ]\n",
    "\n",
    "    # build graph\n",
    "    frame_data = to_graph(frame_data)\n",
    "\n",
    "    return frame_data  # noqa: RET504 explicit return\n",
    "\n",
    "\n",
    "# policy transform\n",
    "def device_cast(frame_data: dict[str, Any], device: torch.device | None = None) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    cast data to device\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\") if device is None else device\n",
    "    if \"graph\" in frame_data[\"observation\"]:\n",
    "        frame_data[\"observation\"][\"graph\"] = frame_data[\"observation\"][\"graph\"].to(device)\n",
    "\n",
    "    return frame_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define policy\n",
    "\n",
    "policy_network = GINPolicyNetwork(dim_feature=6, dim_h=72)\n",
    "value_network = GINValueNetwork(dim_feature=6, dim_h=72)\n",
    "actor = Actor(policy_network=policy_network)\n",
    "critic = Critic(value_network=value_network)\n",
    "wrapped_actor_critic = ActorCritic(\n",
    "    actor=actor, critic=critic, forward_preprocess=device_cast\n",
    ")\n",
    "wrapped_actor_critic = wrapped_actor_critic.to(device)\n",
    "value_estimator = GAE(gamma=gamma, lmbda=lmbda)\n",
    "optimizer = Adam(wrapped_actor_critic.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load state dict\n",
    "# wrapped_actor_critic.load_state_dict(torch.load(\"ppo_parallel_100.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for env rollout\n",
    "\n",
    "rollout_parameters = {\n",
    "    \"policy\": wrapped_actor_critic,\n",
    "    \"env_transform\": env_transform,\n",
    "    \"agent_poses\": agent_poses,\n",
    "    \"num_agents\": num_agents,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"max_steps_per_agent\": max_steps_per_agent,\n",
    "    \"velocity\": velocity,\n",
    "    \"sensor_range\": ray_range,\n",
    "    \"num_rays\": num_rays,\n",
    "    \"min_frontier_pixel\": min_frontier_size,\n",
    "    \"max_frontier_pixel\": max_frontier_size,\n",
    "    \"exploration_threshold\": exploration_threshold,\n",
    "    \"return_maps\": False,\n",
    "    \"requires_grad\": False,\n",
    "}\n",
    "console.print(rollout_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single rollout\n",
    "env = Env(\n",
    "    log_level,\n",
    "    cpp_env_log_path.as_posix(),\n",
    ")\n",
    "rollout_parameters[\"grid_map\"] = random.choice(maps)\n",
    "with torch.no_grad():\n",
    "    single_rollouts = env.rollout(**rollout_parameters)\n",
    "rollouts = single_rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel rollout\n",
    "\n",
    "\n",
    "def rollout_env(env, params):\n",
    "    \"\"\"在给定的环境中执行rollout。\"\"\"\n",
    "    random_index = random.choice(list(range(len(maps))))\n",
    "    # console.print(f\"随机选择地图索引: {random_index}\")\n",
    "    random_map = maps[random_index]\n",
    "\n",
    "    params[\"grid_map\"] = random_map\n",
    "    return env.rollout(**params)\n",
    "\n",
    "\n",
    "def parallel_rollout(envs, num_parallel_envs, rollout_params):\n",
    "    \"\"\"\n",
    "    在多个仿真环境中并行执行rollout。\n",
    "\n",
    "    :param envs: List[object] - 仿真环境列表, 每个环境对象应有一个rollout方法。\n",
    "    :param num_parallel_envs: int - 最大并行环境数量。\n",
    "    :param rollout_params: dict - 传递给每个环境rollout方法的参数。\n",
    "    :return: List - 每个环境的rollout结果列表。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_parallel_envs) as executor:\n",
    "        # 提交所有环境的rollout任务\n",
    "        futures = {executor.submit(rollout_env, env, rollout_params): env for env in envs}\n",
    "\n",
    "        # 收集所有任务的结果\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.extend(result)\n",
    "            except Exception as e:\n",
    "                print(f\"环境 {futures[future]} 的rollout执行时发生错误: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "envs = [Env(cpp_env_log_level, cpp_env_log_path=cpp_env_log_path.as_posix()) for i in range(n_envs)]\n",
    "len(envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel rollout\n",
    "# with torch.no_grad():\n",
    "#     rollouts=parallel_rollout(\n",
    "#         envs=envs,\n",
    "#         num_parallel_envs=n_parallel_envs,\n",
    "#         rollout_params=rollout_parameters,\n",
    "# )\n",
    "# len(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for logging\n",
    "writer = SummaryWriter(tensorboard_dir / \"ppo\")\n",
    "writer.add_text(\"parameters\", str(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "\n",
    "console_logging_data = {}\n",
    "start_time = time.time()\n",
    "for _n_iter in range(n_iters):\n",
    "    ess_weight = entropy_coefficient * entropy_coefficient_decay**_n_iter\n",
    "    # collect data\n",
    "    iter_start_time = time.time()\n",
    "    data_collection_start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        env_rollout_start_time = time.time()\n",
    "        rollouts = parallel_rollout(\n",
    "            envs=envs,\n",
    "            num_parallel_envs=n_parallel_envs,\n",
    "            rollout_params=rollout_parameters,\n",
    "        )\n",
    "        env_rollout_time = time.time() - env_rollout_start_time\n",
    "\n",
    "        data_post_process_start_time = time.time()\n",
    "        # normalize rewards\n",
    "        rollouts = [exploration_reward_rescale(rollout, a=average_val, b=stddev_val) for rollout in rollouts]\n",
    "        rollouts = [delta_time_reward_standardize(rollout, b=b, sigma=sigma, x_star=x_star) for rollout in rollouts]\n",
    "\n",
    "        rollouts = [reward_sum(rollout, gamma=gamma) for rollout in rollouts]\n",
    "        data_post_process_time = time.time() - data_post_process_start_time\n",
    "\n",
    "        gae_start_time = time.time()\n",
    "        # compute GAE\n",
    "        rollouts = [value_estimator(rollout) for rollout in rollouts]\n",
    "        flattened_rollouts = [frame_data for rollout in rollouts for frame_data in rollout]\n",
    "        exploration_rewards = np.mean(\n",
    "            [frame_data[\"next\"][\"reward\"][\"exploration_reward\"] for frame_data in flattened_rollouts]\n",
    "        )\n",
    "        time_step_rewards = np.mean(\n",
    "            [frame_data[\"next\"][\"reward\"][\"time_step_reward\"] for frame_data in flattened_rollouts]\n",
    "        )\n",
    "        total_rewards = np.mean([frame_data[\"next\"][\"reward\"][\"total_reward\"] for frame_data in flattened_rollouts])\n",
    "        actual_frames_per_iter = len(flattened_rollouts)\n",
    "        gae_time = time.time() - gae_start_time\n",
    "        # add to sampler\n",
    "        data_flatten_start_time = time.time()\n",
    "\n",
    "        graphs = []\n",
    "        rewards = []\n",
    "        values = []\n",
    "        advantages = []\n",
    "        log_probs = []\n",
    "        returns = []\n",
    "        total_times = []\n",
    "\n",
    "        for frame_data in flattened_rollouts:\n",
    "            graphs.append(frame_data[\"observation\"][\"graph\"])\n",
    "            rewards.append(frame_data[\"next\"][\"reward\"][\"total_reward\"])\n",
    "            values.append(frame_data[\"value\"])\n",
    "            advantages.append(frame_data[\"advantage\"])\n",
    "            log_probs.append(frame_data[\"log_prob\"])\n",
    "            returns.append(frame_data[\"return\"])\n",
    "            total_times.append(frame_data[\"info\"][\"total_time_step\"])\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        values = torch.tensor(values).to(device)\n",
    "        advantages = torch.tensor(advantages).to(device)\n",
    "        log_probs = torch.tensor(log_probs).to(device)\n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        total_times = torch.tensor(total_times).to(device)\n",
    "\n",
    "        sampler = Sampler(\n",
    "            batch_size=n_frames_per_mini_batch,\n",
    "            length=len(flattened_rollouts),\n",
    "            graphs=graphs,\n",
    "            rewards=rewards,\n",
    "            values=values,\n",
    "            advantages=advantages,\n",
    "            log_probs=log_probs,\n",
    "            returns=returns,\n",
    "            total_times=total_times,\n",
    "        )\n",
    "        data_flatten_time = time.time() - data_flatten_start_time\n",
    "    data_collection_time = time.time() - data_collection_start_time\n",
    "\n",
    "    training_start_time = time.time()\n",
    "    clip_fraction = []\n",
    "    for _n_epoch in range(n_epochs_per_iter):\n",
    "        epoch_start_time = time.time()\n",
    "        n_mini_batches_per_epoch = actual_frames_per_iter // n_frames_per_mini_batch\n",
    "        for _n_mini_batch in range(n_mini_batches_per_epoch):\n",
    "            mini_batch_start_time = time.time()\n",
    "\n",
    "            sample_start_time = time.time()\n",
    "            # sample data\n",
    "            mini_batch_data = sampler.random_sample()\n",
    "            graphs = mini_batch_data[\"graphs\"].to(device)\n",
    "            rewards = mini_batch_data[\"rewards\"]\n",
    "            values = mini_batch_data[\"values\"].to(device).flatten()\n",
    "            advantages = mini_batch_data[\"advantages\"].to(device).flatten()\n",
    "            prev_log_prob = mini_batch_data[\"log_probs\"].to(device).flatten()\n",
    "            returns = mini_batch_data[\"returns\"].to(device).flatten()\n",
    "            total_times = mini_batch_data[\"total_times\"]\n",
    "            frame_indices = mini_batch_data[\"frame_indices\"].to(device)\n",
    "            sample_time = time.time() - sample_start_time\n",
    "\n",
    "            training_data_prepare_start_time = time.time()\n",
    "            # get minibatch data and transform to tensor for training\n",
    "\n",
    "            forward_start_time = time.time()\n",
    "            new_action, new_log_probs, new_values, entropies = wrapped_actor_critic.forward_parallel(\n",
    "                graphs, frame_indices\n",
    "            )\n",
    "            new_log_probs = new_log_probs.to(device).flatten()\n",
    "            new_values = new_values.to(device).flatten()\n",
    "            entropies = entropies.to(device).flatten()\n",
    "\n",
    "            forward_time = time.time() - forward_start_time\n",
    "            data_prepare_time = time.time() - training_data_prepare_start_time\n",
    "            loss_compute_start_time = time.time()\n",
    "\n",
    "            # compute loss\n",
    "            log_ratio = new_log_probs - prev_log_prob\n",
    "            ratio = log_ratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                old_approx_kl = (-log_ratio).mean()\n",
    "                approx_kl = ((ratio - 1) - log_ratio).mean()\n",
    "                clip_fraction += [((ratio - 1.0).abs() > clip_coefficient).float().mean().item()]\n",
    "\n",
    "            pg_loss1 = advantages * ratio\n",
    "            pg_loss2 = advantages * torch.clamp(ratio, 1 - clip_coefficient, 1 + clip_coefficient)\n",
    "            policy_loss = -torch.min(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            ess_loss = -entropies.mean()\n",
    "\n",
    "            value_loss = 0.5 * ((new_values - returns) ** 2).mean()\n",
    "\n",
    "            loss = policy_loss_weight * policy_loss + value_loss_weight * value_loss + ess_weight * ess_loss\n",
    "\n",
    "            loss_compute_time = time.time() - loss_compute_start_time\n",
    "\n",
    "            ### optimizer update ###\n",
    "            optimizer_start_time = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(wrapped_actor_critic.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer_time = time.time() - optimizer_start_time\n",
    "\n",
    "            mini_batch_time = time.time() - mini_batch_start_time\n",
    "\n",
    "        epoch_end_time = time.time() - epoch_start_time\n",
    "        console_logging_data.update(\n",
    "            {\n",
    "                \"policy_loss\": policy_loss.item(),\n",
    "                \"value_loss\": value_loss.item(),\n",
    "                \"ESS\": ess_loss.item(),\n",
    "                \"old_approx_kl\": old_approx_kl.item(),\n",
    "                \"approx_kl\": approx_kl.item(),\n",
    "                \"clip_fraction\": np.mean(clip_fraction),\n",
    "                \"loss\": loss.item(),\n",
    "                \"loss_compute_time\": loss_compute_time,\n",
    "                \"optimizer_time\": optimizer_time,\n",
    "                \"sample_time\": sample_time,\n",
    "                \"forward_time\": forward_time,\n",
    "                \"data_prepare_time\": data_prepare_time,\n",
    "                \"mini_batch_time\": mini_batch_time,\n",
    "                \"data_collection_time\": data_collection_time,\n",
    "                \"env_rollout_time\": env_rollout_time,\n",
    "                \"data_post_process_time\": data_post_process_time,\n",
    "                \"gae_time\": gae_time,\n",
    "                \"mini_batch_size\": f\"{n_mini_batches_per_epoch}/{n_frames_per_mini_batch}\",\n",
    "                \"n_iter\": f\"{_n_iter+1}/{n_iters}\",\n",
    "                \"n_epoch\": f\"{_n_epoch+1}/{n_epochs_per_iter}\",\n",
    "                \"frames_per_iter\": f\"{actual_frames_per_iter}/{n_frames_per_iter}\",\n",
    "                \"epoch_time\": epoch_end_time,\n",
    "                \"data_flatten_time\": data_flatten_time,\n",
    "            }\n",
    "        )\n",
    "        table = Table(title=f\"PPO Training: {n_envs} envs in {n_parallel_envs} threads\")\n",
    "        table.add_column(\"Key\", justify=\"left\")\n",
    "        table.add_column(\"Value\", justify=\"left\")\n",
    "        for k, v in console_logging_data.items():\n",
    "            table.add_row(k, str(v)[:16])\n",
    "        console.print(table)\n",
    "\n",
    "    training_time_per_itr = time.time() - training_start_time\n",
    "\n",
    "    average_exploration_time = torch.mean(total_times.to(torch.float)).item()\n",
    "    episode_reward_mean = torch.mean(returns).item()\n",
    "\n",
    "    console_logging_data.update(\n",
    "        {\n",
    "            \"training_time_per_itr\": training_time_per_itr,\n",
    "            \"average_exploration_time\": average_exploration_time,\n",
    "            \"episode_reward_mean\": episode_reward_mean,\n",
    "            \"total_training_time\": time.time() - start_time,\n",
    "            \"exploration_rewards\": exploration_rewards.item(),\n",
    "            \"time_step_rewards\": time_step_rewards.item(),\n",
    "            \"total_rewards\": total_rewards.item(),\n",
    "            \"ess_weight\": ess_weight,\n",
    "        }\n",
    "    )\n",
    "    global_step = _n_iter\n",
    "    if global_step % 10 == 0 and global_step > 0:\n",
    "        torch.save(\n",
    "            wrapped_actor_critic.state_dict(),\n",
    "            model_dir / f\"ppo_parallel_{global_step}.pt\",\n",
    "        )\n",
    "    writer.add_scalar(\"losses/value_loss\", value_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", policy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/ESS\", ess_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/clip_fraction\", np.mean(clip_fraction), global_step)\n",
    "    writer.add_scalar(\"rewards/average_exploration_time\", average_exploration_time, global_step)\n",
    "    writer.add_scalar(\"rewards/episode_reward_mean\", episode_reward_mean, global_step)\n",
    "    writer.add_scalar(\"rewards/total_reward\", total_rewards.item(), global_step)\n",
    "    writer.add_scalar(\"rewards/exploration_rewards\", exploration_rewards.item(), global_step)\n",
    "    writer.add_scalar(\"rewards/time_step_rewards\", time_step_rewards.item(), global_step)\n",
    "\n",
    "    iter_time = time.time() - iter_start_time\n",
    "    console_logging_data.update(\n",
    "        {\n",
    "            \"iter_time\": iter_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "torch.save(wrapped_actor_critic.state_dict(), model_dir / f\"actor_critic_{global_step}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw raw rewards for debugging\n",
    "\n",
    "mean_val = np.mean(\n",
    "    [frame_data[\"next\"][\"reward\"][\"exploration_reward\"] for rollout in rollouts for frame_data in rollout]\n",
    ")\n",
    "print(f\"exploration reward mean:{mean_val}\")\n",
    "for rollout in rollouts:\n",
    "    plt.scatter(\n",
    "        list(range(len(rollout))),\n",
    "        [frame_data[\"next\"][\"reward\"][\"exploration_reward\"] for frame_data in rollout],\n",
    "    )\n",
    "plt.show()\n",
    "mean_val = np.mean([frame_data[\"next\"][\"reward\"][\"time_step_reward\"] for rollout in rollouts for frame_data in rollout])\n",
    "print(f\"time_step_reward mean: {mean_val}\")\n",
    "for rollout in rollouts:\n",
    "    plt.scatter(\n",
    "        list(range(len(rollout))),\n",
    "        [frame_data[\"next\"][\"reward\"][\"time_step_reward\"] for frame_data in rollout],\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_rewards = np.array(\n",
    "    [frame_data[\"next\"][\"reward\"][\"exploration_reward\"] for rollout in rollouts for frame_data in rollout]\n",
    ")\n",
    "time_step_rewards = np.array(\n",
    "    [frame_data[\"next\"][\"reward\"][\"time_step_reward\"] for rollout in rollouts for frame_data in rollout]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalize rewards\n",
    "rollouts = [\n",
    "    exploration_reward_rescale(rollout, a=a)\n",
    "    for rollout in rollouts\n",
    "]\n",
    "rollouts = [delta_time_reward_standardize(rollout,b=b,sigma=sigma,x_star=x_star) for rollout in rollouts]\n",
    "rollouts = [reward_sum(rollout,gamma=gamma) for rollout in rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute GAE\n",
    "from rl.actor_critic import GAE\n",
    "\n",
    "gae = GAE(gamma=gamma, lmbda=lmbda)\n",
    "rollouts = [gae(rollout) for rollout in rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw normalized rewards for debugging\n",
    "\n",
    "# total_reward\n",
    "for rollout in rollouts:\n",
    "    plt.plot([frame_data[\"next\"][\"reward\"][\"total_reward\"] for frame_data in rollout])\n",
    "mean_val = np.mean(\n",
    "    [\n",
    "        frame_data[\"next\"][\"reward\"][\"total_reward\"]\n",
    "        for rollout in rollouts\n",
    "        for frame_data in rollout\n",
    "    ]\n",
    ")\n",
    "plt.title(f\"total reward:{mean_val}\")\n",
    "plt.axhline(mean_val, color=\"r\", linestyle=\"--\")  # type: ignore  # noqa: PGH003\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# exploration_reward\n",
    "for rollout in rollouts:\n",
    "    plt.plot(\n",
    "        [frame_data[\"next\"][\"reward\"][\"exploration_reward\"] for frame_data in rollout]\n",
    "    )\n",
    "mean_val = np.mean(\n",
    "    [\n",
    "        frame_data[\"next\"][\"reward\"][\"exploration_reward\"]\n",
    "        for rollout in rollouts\n",
    "        for frame_data in rollout\n",
    "    ]\n",
    ")\n",
    "plt.title(f\"exploration reward:{mean_val}\")\n",
    "plt.axhline(mean_val, color=\"r\", linestyle=\"--\")  # type: ignore  # noqa: PGH003\n",
    "plt.show()\n",
    "\n",
    "# time_step_reward\n",
    "for rollout in rollouts:\n",
    "    plt.plot(\n",
    "        [frame_data[\"next\"][\"reward\"][\"time_step_reward\"] for frame_data in rollout]\n",
    "    )\n",
    "mean_val = np.mean(\n",
    "    [\n",
    "        frame_data[\"next\"][\"reward\"][\"time_step_reward\"]\n",
    "        for rollout in rollouts\n",
    "        for frame_data in rollout\n",
    "    ]\n",
    ")\n",
    "plt.title(f\"time step reward:{mean_val}\")\n",
    "plt.axhline(mean_val, color=\"r\", linestyle=\"--\")  # type: ignore  # noqa: PGH003\n",
    "plt.show()\n",
    "\n",
    "# gae\n",
    "for rollout in rollouts:\n",
    "    gaes = [frame_data[\"advantage\"].item() for frame_data in rollout]\n",
    "    plt.plot(gaes)\n",
    "plt.title(f\"gae:{np.mean(gaes)}\")\n",
    "plt.axhline(y=np.mean(gaes), color=\"r\", linestyle=\"--\")  # type: ignore  # noqa: PGH003\n",
    "plt.show()\n",
    "\n",
    "# reward_mean\n",
    "reward_mean = [\n",
    "    np.mean(\n",
    "        [\n",
    "            frame_data[\"advantage\"].item() + frame_data[\"value\"].item()\n",
    "            for frame_data in rollout\n",
    "        ]\n",
    "    )\n",
    "    for rollout in rollouts\n",
    "]\n",
    "plt.scatter(list(range(len(reward_mean))), reward_mean)\n",
    "plt.boxplot(reward_mean)\n",
    "\n",
    "plt.title(f\"reward mean:{np.mean(reward_mean)}\")\n",
    "plt.axhline(y=np.mean(reward_mean), color=\"r\", linestyle=\"--\")  # type: ignore  # noqa: PGH003\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_mean[329]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rollouts = single_env_rollout(env, map_img, policy=wrapped_actor_critic,env_transform=env_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store maps to gif\n",
    "map_path = \"map/medium/12.png\"\n",
    "for rollout in rollouts:\n",
    "    agent_id = rollout[0][\"info\"][\"agent_id\"]\n",
    "    imgs = [Image.fromarray(frame_data[\"observation\"][\"agent_map\"]) for frame_data in rollout]\n",
    "    imgs[0].save(\n",
    "        output_images_dir / f\"agent_{agent_id}_{map_path.split('/')[-1]}.gif\",\n",
    "        save_all=True,\n",
    "        append_images=imgs[1:],\n",
    "        duration=1000,\n",
    "        loop=0,\n",
    "    )\n",
    "flattened_rollouts = []\n",
    "for rollout in rollouts:\n",
    "    flattened_rollouts.extend(rollout)\n",
    "sorted_rollouts = sorted(flattened_rollouts, key=lambda x: x[\"info\"][\"step_cnt\"])\n",
    "imgs = [Image.fromarray(frame_data[\"observation\"][\"global_map\"]) for frame_data in sorted_rollouts]\n",
    "imgs[0].save(\n",
    "    output_images_dir / f\"global_map_{map_path.split('/')[-1]}.gif\",\n",
    "    save_all=True,\n",
    "    append_images=imgs[1:],\n",
    "    duration=1000,\n",
    "    loop=0,\n",
    ")\n",
    "rollouts[2][-1][\"info\"][\"total_time_step\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(\n",
    "    [\n",
    "        np.count_nonzero(rollouts[0][-1][\"observation\"][\"agent_map\"] != 127),\n",
    "        np.count_nonzero(rollouts[1][-1][\"observation\"][\"agent_map\"] != 127),\n",
    "        np.count_nonzero(rollouts[2][-1][\"observation\"][\"agent_map\"] != 127),\n",
    "    ]\n",
    ") / np.count_nonzero(rollouts[0][-1][\"observation\"][\"global_map\"] != 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force clean\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "map_img=np.ones_like(map_img)*255\n",
    "map_img[50:75,:]=0\n",
    "map_img[:,50:75]=0\n",
    "map_img[125:150,:]=0\n",
    "map_img[:,125:150]=0\n",
    "plt.imshow(map_img, cmap=\"gray\", vmin=0, vmax=255)\n",
    "\n",
    "# tests\n",
    "env=Env()\n",
    "\n",
    "\n",
    "\n",
    "from mcerl import GridMap\n",
    "test_env_grid_map=GridMap(map_img)\n",
    "map_to_update=map_img.copy()\n",
    "map_to_update.fill(127)\n",
    "test_map_tp_update=GridMap(map_to_update)\n",
    "basic_end_points, circle_end_points, circle_end_points_with_polygon, map_to_update,roi_map = (\n",
    "    env._env.test_map_update(\n",
    "        test_env_grid_map, test_map_tp_update, (100, 100), 30, 32, 5\n",
    "    )\n",
    ")\n",
    "a=np.array(map_to_update)\n",
    "b=np.array(roi_map)\n",
    "basic_end_points = [(x / 10000, y / 10000) for x, y in basic_end_points]\n",
    "plt.imshow(b, cmap=\"gray\", vmin=0, vmax=255)\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "plt.imshow(map_img, cmap=\"gray\", vmin=0, vmax=255)\n",
    "# plt.scatter(*zip(*circle_end_points), c=\"g\", s=2)\n",
    "# plt.scatter(*zip(*basic_end_points), c=\"b\", s=2)\n",
    "plt.scatter(*zip(*circle_end_points_with_polygon), c=\"r\", s=2,marker='x')\n",
    "rect=[(70, 70), (131, 131)]\n",
    "rect=Rectangle((rect[0][0],rect[0][1]),rect[1][0]-rect[0][0],rect[1][1]-rect[0][1],linewidth=1,edgecolor='y',facecolor='none')\n",
    "plt.gca().add_patch(rect)\n",
    "bbx = cv2.boundingRect(np.array(circle_end_points_with_polygon))\n",
    "np.array(circle_end_points_with_polygon) - bbx[0:2]\n",
    "# poly=cv2.fillPoly(b, [np.array(circle_end_points_with_polygon) - bbx[0:2]], 255)\n",
    "# plt.imshow(poly, cmap=\"gray\", vmin=0, vmax=255)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
