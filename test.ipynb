{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import annotations\n",
    "\n",
    "import datetime\n",
    "import pathlib as pl\n",
    "import random\n",
    "import shutil\n",
    "import string\n",
    "import time\n",
    "from math import pi\n",
    "from typing import Any\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "from rl.actor_critic import GAE, Actor, ActorCritic, Critic\n",
    "from rl.network import GINPolicyNetwork, GINValueNetwork\n",
    "from rl.utils import Sampler, to_graph\n",
    "from torch.optim.adam import Adam\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from mcerl.env import Env\n",
    "from mcerl.utils import (\n",
    "    delta_time_reward_standardize,\n",
    "    exploration_reward_rescale,\n",
    "    reward_sum,\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load map\n",
    "map_path = \"map/0.png\"\n",
    "img = Image.open(map_path)\n",
    "img = ImageOps.grayscale(img)\n",
    "img = img.resize((200, 200))\n",
    "grid_map = np.array(img)\n",
    "grid_map[grid_map < 100] = 0\n",
    "grid_map[grid_map >= 100] = 255\n",
    "plt.imshow(grid_map, cmap=\"gray\", vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "#########################\n",
    "# log参数\n",
    "#########################\n",
    "\n",
    "# workspace\n",
    "workspace_dir = pl.Path.cwd()\n",
    "# output\n",
    "output_dir = pl.Path(workspace_dir) / \"output\"\n",
    "current_date = datetime.datetime.now().strftime(\"%Y_%m_%d_%H-%M-%S\")\n",
    "random_string = \"\".join(random.choices(string.ascii_letters + string.digits, k=6))\n",
    "session_name = f\"{current_date}_{random_string}\"\n",
    "# experiment\n",
    "experiment_dir = output_dir / session_name\n",
    "log_dir = experiment_dir / \"log\"\n",
    "model_dir = experiment_dir / \"model\"\n",
    "output_images_dir = experiment_dir / \"images\"\n",
    "tensorboard_dir = experiment_dir / \"tensorboard\"\n",
    "\n",
    "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "tensorboard_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "#########################\n",
    "# 环境参数\n",
    "#########################\n",
    "\n",
    "\n",
    "exclude_parameters = list(locals().keys())\n",
    "\n",
    "# log level\n",
    "log_level = \"warning\"\n",
    "\n",
    "# 几个agent\n",
    "num_agents = 3\n",
    "\n",
    "# agent的初始位置\n",
    "agent_poses = None\n",
    "\n",
    "# agent的初始方向\n",
    "num_rays = 32\n",
    "\n",
    "# 一个env最多迭代多少步\n",
    "max_steps = 10000\n",
    "\n",
    "# 一个agent最多迭代多少步\n",
    "max_steps_per_agent = 100\n",
    "\n",
    "# 传感器的范围\n",
    "ray_range = 30\n",
    "\n",
    "# 速度(pixel per step)\n",
    "velocity = 1\n",
    "\n",
    "# 最小的frontier pixel数\n",
    "min_frontier_size = 8\n",
    "\n",
    "# 最大的frontier pixel数\n",
    "max_frontier_size = 30\n",
    "\n",
    "# 探索的阈值\n",
    "exploration_threshold = 0.95\n",
    "\n",
    "# 地图的高度和宽度\n",
    "map_height, map_width = grid_map.shape\n",
    "\n",
    "# 一个frontier最多可以获得多少信息增益\n",
    "max_exploration_gain = ray_range**2 * pi / 2.0\n",
    "\n",
    "\n",
    "#########################\n",
    "# PPO参数\n",
    "#########################\n",
    "\n",
    "# gae权重\n",
    "lmbda = 0.95\n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# clip范围\n",
    "clip_coefficient = 0.2\n",
    "\n",
    "# 最大gradient范数\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "# ESS\n",
    "entropy_weight = 0.001\n",
    "\n",
    "# policy loss的权重\n",
    "policy_loss_weight = 1.0\n",
    "\n",
    "# value loss的权重\n",
    "value_loss_weight = 0.5\n",
    "\n",
    "#########################\n",
    "# 训练参数\n",
    "#########################\n",
    "\n",
    "# 设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 总共训练的次数\n",
    "n_iters = 1000\n",
    "\n",
    "# 每次训练的frame数\n",
    "n_frames_per_iter = 10000\n",
    "\n",
    "# 每次训练的并行环境数(num worker)\n",
    "n_parallel_envs = 15\n",
    "\n",
    "# 每次训练的epoch数, 即数据要被训练多少次\n",
    "n_epochs_per_iter = 5\n",
    "\n",
    "# 每次epoch的mini_batch大小\n",
    "mini_batch_size = n_frames_per_iter // 10\n",
    "\n",
    "# 每个agent的最大步数\n",
    "max_steps_per_agent = 50\n",
    "\n",
    "# 每次训练所用到的总的环境数量\n",
    "n_envs_per_iter = round(n_frames_per_iter / num_agents / max_steps_per_agent)\n",
    "\n",
    "# 每个epoch的frame数\n",
    "n_frames_per_epoch = n_frames_per_iter * n_epochs_per_iter\n",
    "\n",
    "# 每个epoch的mini_batch数\n",
    "n_minibatches_per_epoch = n_frames_per_epoch // mini_batch_size\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    k: v\n",
    "    for k, v in locals().items()\n",
    "    if k not in [*exclude_parameters, \"exclude_parameters\"]\n",
    "}\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform function\n",
    "# env_transform\n",
    "def env_transform(frame_data: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    normalize position, exploration gain,etc.\n",
    "    \"\"\"\n",
    "    # normalize frontier position to [0,1] and exploration gain to [0,1]\n",
    "    width = float(map_width)\n",
    "    height = float(map_height)\n",
    "    frame_data[\"observation\"][\"frontier_points\"] = [\n",
    "        (\n",
    "            float(x) / width,\n",
    "            float(y) / height,\n",
    "            float(gain) / max_exploration_gain,\n",
    "            float(distance) / (width + height),\n",
    "        )\n",
    "        for x, y, gain, distance in frame_data[\"observation\"][\"frontier_points\"]\n",
    "    ]\n",
    "    # normalize position to [0,1]\n",
    "    frame_data[\"observation\"][\"pos\"] = [\n",
    "        (float(x) / width, float(y) / height)\n",
    "        for x, y in frame_data[\"observation\"][\"pos\"]\n",
    "    ]\n",
    "    frame_data[\"observation\"][\"target_pos\"] = [\n",
    "        (float(x) / width, float(y) / height)\n",
    "        for x, y in frame_data[\"observation\"][\"target_pos\"]\n",
    "    ]\n",
    "    # build graph\n",
    "    frame_data = to_graph(frame_data)\n",
    "    return frame_data  # noqa: RET504 explicit return \n",
    "# policy transform\n",
    "def device_cast(\n",
    "    frame_data: dict[str, Any], device: torch.device | None = None\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    cast data to device\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\") if device is None else device\n",
    "    frame_data[\"observation\"][\"graph\"] = frame_data[\"observation\"][\"graph\"].to(device)\n",
    "\n",
    "    return frame_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define policy\n",
    "\n",
    "policy_network = GINPolicyNetwork(dim_feature=6, dim_h=32)\n",
    "value_network = GINValueNetwork(dim_feature=6, dim_h=32)\n",
    "actor = Actor(policy_network=policy_network)\n",
    "critic = Critic(value_network=value_network)\n",
    "wrapped_actor_critic = ActorCritic(\n",
    "    actor=actor, critic=critic, forward_preprocess=device_cast\n",
    ")\n",
    "wrapped_actor_critic = wrapped_actor_critic.to(device)\n",
    "value_estimator = GAE(gamma=gamma, lmbda=lmbda)\n",
    "optimizer = Adam(wrapped_actor_critic.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapped_actor_critic.load_state_dict(torch.load(\"ppo_parallel_100.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for env rollout\n",
    "\n",
    "\n",
    "rollout_parameters = {\n",
    "    \"grid_map\": grid_map,\n",
    "    \"policy\": wrapped_actor_critic,\n",
    "    \"env_transform\": env_transform,\n",
    "    \"agent_poses\": agent_poses,\n",
    "    \"num_agents\": num_agents,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"max_steps_per_agent\": max_steps_per_agent,\n",
    "    \"velocity\": velocity,\n",
    "    \"sensor_range\": ray_range,\n",
    "    \"num_rays\": num_rays,\n",
    "    \"min_frontier_pixel\": min_frontier_size,\n",
    "    \"max_frontier_pixel\": max_frontier_size,\n",
    "    \"exploration_threshold\": exploration_threshold,\n",
    "    \"return_maps\": True,\n",
    "    \"requires_grad\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single rollout\n",
    "env = Env(\n",
    "    log_level,\n",
    "    (log_dir / \"output.log\").as_posix(),\n",
    ")\n",
    "with torch.no_grad():\n",
    "    single_rollouts = env.rollout(**rollout_parameters)\n",
    "rollouts = single_rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel rollout\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "def rollout_env(env, params):\n",
    "    \"\"\"在给定的环境中执行rollout。\"\"\"\n",
    "    return env.rollout(**params)\n",
    "\n",
    "\n",
    "def parallel_rollout(envs, num_parallel_envs, rollout_params):\n",
    "    \"\"\"\n",
    "    在多个仿真环境中并行执行rollout。\n",
    "\n",
    "    :param envs: List[object] - 仿真环境列表，每个环境对象应有一个rollout方法。\n",
    "    :param num_parallel_envs: int - 最大并行环境数量。\n",
    "    :param rollout_params: dict - 传递给每个环境rollout方法的参数。\n",
    "    :return: List - 每个环境的rollout结果列表。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(\n",
    "        max_workers=num_parallel_envs\n",
    "    ) as executor:\n",
    "        # 提交所有环境的rollout任务\n",
    "        futures = {\n",
    "            executor.submit(rollout_env, env, rollout_params): env for env in envs\n",
    "        }\n",
    "\n",
    "        # 收集所有任务的结果\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.extend(result)\n",
    "            except Exception as e:\n",
    "                print(f\"环境 {futures[future]} 的rollout执行时发生错误: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "envs = [Env(log_level, log_path=(log_dir / \"output.log\").as_posix()) for i in range(20)]\n",
    "with torch.no_grad():\n",
    "    rollouts = parallel_rollout(envs, 10, rollout_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    single_rollouts = env.rollout(**parameters)\n",
    "rollouts = single_rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for logging\n",
    "timestamp_str = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "writer = SummaryWriter(f\"runs/ppo{timestamp_str}\")\n",
    "writer.add_text(\"parameters\", str(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "for _n_iter in tqdm.tqdm(range(n_iters)):\n",
    "    # collect data\n",
    "    iter_start_time = time.time()\n",
    "    data_collection_start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        ##################\n",
    "        # BEGIN DEBUGGING\n",
    "        ##################\n",
    "        # rollouts = single_env_rollout(env, grid_map, policy=wrapped_actor_critic,env_transform=env_transform)\n",
    "        ##################\n",
    "        # END DEBUGGING\n",
    "        ##################\n",
    "        env_rollout_start_time = time.time()\n",
    "        rollouts = multi_threaded_rollout(\n",
    "            env=lambda: Env(\n",
    "                num_agents=num_agents,\n",
    "                max_steps=max_steps,\n",
    "                max_steps_per_agent=max_steps_per_agent,\n",
    "                velocity=velocity,\n",
    "                sensor_range=ray_range,\n",
    "                num_rays=num_rays,\n",
    "                min_frontier_pixel=min_frontier_size,\n",
    "                max_frontier_pixel=max_frontier_size,\n",
    "            ),\n",
    "            grid_map=grid_map,\n",
    "            agent_poses=agent_poses,\n",
    "            policy=wrapped_actor_critic,\n",
    "            num_threads=n_parallel_envs,\n",
    "            epochs=n_envs_per_iter,\n",
    "            return_maps=False,\n",
    "            env_transform=env_transform,\n",
    "        )\n",
    "        env_rollout_end_time = time.time()\n",
    "        # print(f\"env rollout time: {env_rollout_end_time - env_rollout_start_time}\")\n",
    "\n",
    "        data_post_process_start_time = time.time()\n",
    "        # normalize rewards\n",
    "        rollouts = [\n",
    "            exploration_reward_rescale(rollout, max_value=max_exploration_gain)\n",
    "            for rollout in rollouts\n",
    "        ]\n",
    "        rollouts = [delta_time_reward_standardize(rollout) for rollout in rollouts]\n",
    "        rollouts = [reward_sum(rollout, gamma=gamma) for rollout in rollouts]\n",
    "        data_post_process_end_time = time.time()\n",
    "        # print(\n",
    "        # f\"data post process time: {data_post_process_end_time - data_post_process_start_time}\"\n",
    "        # )\n",
    "\n",
    "        gae_start_time = time.time()\n",
    "        # compute GAE\n",
    "        rollouts = [value_estimator(rollout) for rollout in rollouts]\n",
    "        flattened_rollouts = [\n",
    "            frame_data for rollout in rollouts for frame_data in rollout\n",
    "        ]\n",
    "        gae_end_time = time.time()\n",
    "        # print(f\"GAE time: {gae_end_time - gae_start_time}\")\n",
    "        # add to sampler\n",
    "\n",
    "        # frame_indices = None\n",
    "        graphs = []\n",
    "        rewards = []\n",
    "        values = []\n",
    "        advantages = []\n",
    "        log_probs = []\n",
    "        returns = []\n",
    "        total_times = []\n",
    "\n",
    "        for frame_data in flattened_rollouts:\n",
    "            graphs.append(frame_data[\"observation\"][\"graph\"])\n",
    "            rewards.append(frame_data[\"next\"][\"reward\"][\"total_reward\"])\n",
    "            values.append(frame_data[\"value\"])\n",
    "            advantages.append(frame_data[\"advantage\"])\n",
    "            log_probs.append(frame_data[\"log_prob\"])\n",
    "            returns.append(frame_data[\"return\"])\n",
    "            total_times.append(frame_data[\"info\"][\"total_time_step\"])\n",
    "            # if frame_indices is None:\n",
    "            #     frame_indices = torch.zeros(graph.num_graphs)\n",
    "            # else:\n",
    "            #     frame_indices = torch.cat(\n",
    "            #         [\n",
    "            #             frame_indices,\n",
    "            #             torch.ones(graph.num_graphs) * (frame_indices.max() + 1),\n",
    "            #         ]\n",
    "            #     )\n",
    "        #     for i in range(graph.num_graphs):\n",
    "        #         graphs.append(graph[i])\n",
    "\n",
    "        # graphs = DataLoader(\n",
    "        #     graphs,\n",
    "        #     batch_size=len(graphs),\n",
    "        # )\n",
    "        # graphs = next(iter(graphs))\n",
    "        # graphs = graphs.to(device)\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        values = torch.tensor(values).to(device)\n",
    "        advantages = torch.tensor(advantages).to(device)\n",
    "        log_probs = torch.tensor(log_probs).to(device)\n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        total_times = torch.tensor(total_times).to(device)\n",
    "\n",
    "        sampler = Sampler(\n",
    "            batch_size=mini_batch_size,\n",
    "            length=len(flattened_rollouts),\n",
    "            graphs=graphs,\n",
    "            rewards=rewards,\n",
    "            values=values,\n",
    "            advantages=advantages,\n",
    "            log_probs=log_probs,\n",
    "            returns=returns,\n",
    "            total_times=total_times,\n",
    "        )\n",
    "    data_collection_end_time = time.time()\n",
    "    # print(\n",
    "    # f\"data collection time: {data_collection_end_time - data_collection_start_time}\"\n",
    "    # )\n",
    "    ##################\n",
    "    # BEGIN DEBUGGING\n",
    "    ##################\n",
    "    # sampler = Sampler(\n",
    "    #     rollout=[frame_data for rollout in rollouts for frame_data in rollout],\n",
    "    #     batch_size=10,\n",
    "    # )\n",
    "    ##################\n",
    "    # END DEBUGGING\n",
    "    ##################\n",
    "    # epoch loop\n",
    "    clip_fraction = []\n",
    "    ##################\n",
    "    # BEGIN DEBUGGING\n",
    "    ##################\n",
    "    # for _n_epoch in range(1):\n",
    "    ##################\n",
    "    # END DEBUGGING\n",
    "    ##################\n",
    "    training_start_time = time.time()\n",
    "    for _n_epoch in range(n_epochs_per_iter):\n",
    "        epoch_start_time = time.time()\n",
    "        ##################\n",
    "        # BEGIN DEBUGGING\n",
    "        ##################\n",
    "        # for _n_mini_batch in range(1):\n",
    "        ##################\n",
    "        # END DEBUGGING\n",
    "        ##################\n",
    "        for _n_mini_batch in range(n_frames_per_iter // mini_batch_size):\n",
    "            mini_batch_start_time = time.time()\n",
    "\n",
    "            sample_start_time = time.time()\n",
    "            # sample data\n",
    "            mini_batch_data = sampler.random_sample()\n",
    "            graphs = mini_batch_data[\"graphs\"].to(device)\n",
    "            rewards = mini_batch_data[\"rewards\"]\n",
    "            values = mini_batch_data[\"values\"].to(device).flatten()\n",
    "            advantages = mini_batch_data[\"advantages\"].to(device).flatten()\n",
    "            prev_log_prob = mini_batch_data[\"log_probs\"].to(device).flatten()\n",
    "            returns = mini_batch_data[\"returns\"].to(device).flatten()\n",
    "            total_times = mini_batch_data[\"total_times\"]\n",
    "            frame_indices = mini_batch_data[\"frame_indices\"].to(device)\n",
    "            sample_end_time = time.time()\n",
    "            # print(f\"sample time: {sample_end_time - sample_start_time}\")\n",
    "\n",
    "            training_data_prepare_start_time = time.time()\n",
    "            # get minibatch data and transform to tensor for training\n",
    "\n",
    "            forward_start_time = time.time()\n",
    "            new_action, new_log_probs, new_values, entropy = (\n",
    "                wrapped_actor_critic.forward_parallel(graphs, frame_indices)\n",
    "            )\n",
    "            new_log_probs = new_log_probs.to(device).flatten()\n",
    "            new_values = new_values.to(device).flatten()\n",
    "            forward_end_time = time.time()\n",
    "            # print(f\"forward time: {forward_end_time - forward_start_time}\")\n",
    "\n",
    "            training_data_prepare_end_time = time.time()\n",
    "            # print(\n",
    "            # f\"training data prepare time: {training_data_prepare_end_time - training_data_prepare_start_time}\"\n",
    "            # )\n",
    "            loss_compute_start_time = time.time()\n",
    "            # compute loss\n",
    "            log_ratio = new_log_probs - prev_log_prob\n",
    "            ratio = log_ratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                old_approx_kl = (-log_ratio).mean()\n",
    "                approx_kl = ((ratio - 1) - log_ratio).mean()\n",
    "                clip_fraction += [\n",
    "                    ((ratio - 1.0).abs() > clip_coefficient).float().mean().item()\n",
    "                ]\n",
    "\n",
    "            pg_loss1 = -advantages * ratio\n",
    "            pg_loss2 = -advantages * torch.clamp(\n",
    "                ratio, 1 - clip_coefficient, 1 + clip_coefficient\n",
    "            )\n",
    "\n",
    "            policy_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "            value_loss = 0.5 * ((new_values - returns) ** 2).mean()\n",
    "            ess_loss = entropy.mean()\n",
    "            loss = (\n",
    "                policy_loss_weight * policy_loss\n",
    "                + value_loss_weight * value_loss\n",
    "                - entropy_weight * ess_loss\n",
    "            )\n",
    "            loss_compute_end_time = time.time()\n",
    "            # print(\n",
    "            # f\"loss compute time: {loss_compute_end_time - loss_compute_start_time}\"\n",
    "            # )\n",
    "\n",
    "            optimizer_start_time = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                wrapped_actor_critic.parameters(), max_grad_norm\n",
    "            )\n",
    "            optimizer.step()\n",
    "            optimizer_end_time = time.time()\n",
    "            # print(f\"optimizer time: {optimizer_end_time - optimizer_start_time}\")\n",
    "            mini_batch_end_time = time.time()\n",
    "            # print(f\"mini batch time: {mini_batch_end_time - mini_batch_start_time}\")\n",
    "        epoch_end_time = time.time()\n",
    "        # print(f\"epoch time: {epoch_end_time - epoch_start_time}\")\n",
    "    training_end_time = time.time()\n",
    "    # print(f\"training time: {training_end_time - training_start_time}\")\n",
    "\n",
    "    average_epsiode_time = torch.mean(total_times.to(torch.float)).item()\n",
    "    episode_reward_mean = torch.mean(returns).item()\n",
    "\n",
    "    global_step = _n_iter\n",
    "    if global_step % 10 == 0 and global_step > 0:\n",
    "        torch.save(wrapped_actor_critic.state_dict(), f\"ppo_parallel_{global_step}.pt\")\n",
    "    writer.add_scalar(\"losses/value_loss\", value_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", policy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/ESS\", ess_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/clip_fraction\", np.mean(clip_fraction), global_step)\n",
    "    writer.add_scalar(\"info/episode_reward_mean\", episode_reward_mean, global_step)\n",
    "    writer.add_scalar(\"info/average_epsiode_time\", average_epsiode_time, global_step)\n",
    "    iter_end_time = time.time()\n",
    "    # print(f\"iter time: {iter_end_time - iter_start_time}\")\n",
    "\n",
    "torch.save(wrapped_actor_critic.state_dict(), f\"ppo_parallel_{global_step}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw raw rewards for debugging\n",
    "\n",
    "#print(\"exploration reward:\")\n",
    "for rollout in rollouts:\n",
    "    plt.plot(\n",
    "        [frame_data[\"next\"][\"reward\"][\"exploration_reward\"] for frame_data in rollout]\n",
    "    )\n",
    "plt.show()\n",
    "#print(\"time_step_reward :\")\n",
    "for rollout in rollouts:\n",
    "    plt.plot(\n",
    "        [frame_data[\"next\"][\"reward\"][\"time_step_reward\"] for frame_data in rollout]\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbda = 0.95\n",
    "gamma = 0.99\n",
    "# normalize rewards\n",
    "rollouts = [\n",
    "    exploration_reward_rescale(rollout, max_value=max_exploration_gain)\n",
    "    for rollout in rollouts\n",
    "]\n",
    "rollouts = [delta_time_reward_standardize(rollout) for rollout in rollouts]\n",
    "rollouts = [reward_sum(rollout,gamma=gamma) for rollout in rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute GAE\n",
    "from rl.actor_critic import GAE\n",
    "\n",
    "gae = GAE(gamma=gamma, lmbda=lmbda)\n",
    "rollouts = [gae(rollout) for rollout in rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw normalized rewards for debugging\n",
    "# total_reward\n",
    "for rollout in rollouts:\n",
    "    plt.plot([frame_data[\"next\"][\"reward\"][\"total_reward\"] for frame_data in rollout])\n",
    "mean_val = np.mean(\n",
    "    [\n",
    "        frame_data[\"next\"][\"reward\"][\"total_reward\"]\n",
    "        for frame_data in rollout\n",
    "        for rollout in rollouts\n",
    "    ]\n",
    ")\n",
    "plt.title(f\"total reward:{mean_val}\")\n",
    "plt.axhline(mean_val, color=\"r\", linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# exploration_reward\n",
    "for rollout in rollouts:\n",
    "    plt.plot(\n",
    "        [frame_data[\"next\"][\"reward\"][\"exploration_reward\"] for frame_data in rollout]\n",
    "    )\n",
    "mean_val = np.mean(\n",
    "    [\n",
    "        frame_data[\"next\"][\"reward\"][\"exploration_reward\"]\n",
    "        for frame_data in rollout\n",
    "        for rollout in rollouts\n",
    "    ]\n",
    ")\n",
    "plt.title(f\"exploration reward:{mean_val}\")\n",
    "plt.axhline(mean_val, color=\"r\", linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "# time_step_reward\n",
    "for rollout in rollouts:\n",
    "    plt.plot(\n",
    "        [frame_data[\"next\"][\"reward\"][\"time_step_reward\"] for frame_data in rollout]\n",
    "    )\n",
    "mean_val = np.mean(\n",
    "    [\n",
    "        frame_data[\"next\"][\"reward\"][\"time_step_reward\"]\n",
    "        for frame_data in rollout\n",
    "        for rollout in rollouts\n",
    "    ]\n",
    ")\n",
    "plt.title(f\"time step reward:{mean_val}\")\n",
    "plt.axhline(mean_val, color=\"r\", linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "# reward_to_go\n",
    "for rollout in rollouts:\n",
    "    plt.plot([frame_data[\"reward_to_go\"] for frame_data in rollout])\n",
    "mean_val = np.mean(\n",
    "    [frame_data[\"reward_to_go\"] for frame_data in rollout for rollout in rollouts]\n",
    ")\n",
    "plt.title(f\"reward to go:{mean_val}\")\n",
    "plt.axhline(mean_val, color=\"r\", linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# reward_mean\n",
    "reward_mean = [\n",
    "    np.mean([frame_data[\"reward_to_go\"] for frame_data in rollout])\n",
    "    for rollout in rollouts\n",
    "]\n",
    "plt.plot(reward_mean)\n",
    "plt.title(f\"reward mean:{np.mean(reward_mean)}\")\n",
    "plt.axhline(y=np.mean(reward_mean), color=\"r\", linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "# gae\n",
    "gaes = [frame_data[\"advantage\"].item() for frame_data in rollout for rollout in rollouts]\n",
    "plt.plot(gaes)\n",
    "plt.title(f\"gae:{np.mean(gaes)}\")\n",
    "plt.axhline(y=np.mean(gaes), color=\"r\", linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts = single_env_rollout(env, grid_map, policy=wrapped_actor_critic,env_transform=env_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store maps to gif\n",
    "for rollout in rollouts:\n",
    "    agent_id = rollout[0][\"info\"][\"agent_id\"]\n",
    "    imgs = [\n",
    "        Image.fromarray(frame_data[\"observation\"][\"agent_map\"])\n",
    "        for frame_data in rollout\n",
    "    ]\n",
    "    imgs[0].save(\n",
    "        output_images_dir / f\"agent_{agent_id}_{map_path.split('/')[-1]}.gif\",\n",
    "        save_all=True,\n",
    "        append_images=imgs[1:],\n",
    "        duration=1000,\n",
    "        loop=0,\n",
    "    )\n",
    "flattened_rollouts = []\n",
    "for rollout in rollouts:\n",
    "    flattened_rollouts.extend(rollout)\n",
    "sorted_rollouts = sorted(flattened_rollouts, key=lambda x: x[\"info\"][\"step_cnt\"])\n",
    "imgs = [\n",
    "    Image.fromarray(frame_data[\"observation\"][\"global_map\"])\n",
    "    for frame_data in sorted_rollouts\n",
    "]\n",
    "imgs[0].save(\n",
    "    output_images_dir / f\"global_map_{map_path.split('/')[-1]}.gif\",\n",
    "    save_all=True,\n",
    "    append_images=imgs[1:],\n",
    "    duration=1000,\n",
    "    loop=0,\n",
    ")\n",
    "rollouts[2][-1][\"info\"][\"total_time_step\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(\n",
    "    [\n",
    "        np.count_nonzero(rollouts[0][-1][\"observation\"][\"agent_map\"] != 127),\n",
    "        np.count_nonzero(rollouts[1][-1][\"observation\"][\"agent_map\"] != 127),\n",
    "        np.count_nonzero(rollouts[2][-1][\"observation\"][\"agent_map\"] != 127),\n",
    "    ]\n",
    ") / np.count_nonzero(rollouts[0][-1][\"observation\"][\"global_map\"] != 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force clean\n",
    "\n",
    "\n",
    "\n",
    "shutil.rmtree(experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "grid_map=np.ones_like(grid_map)*255\n",
    "grid_map[50:75,:]=0\n",
    "grid_map[:,50:75]=0\n",
    "grid_map[125:150,:]=0\n",
    "grid_map[:,125:150]=0\n",
    "plt.imshow(grid_map, cmap=\"gray\", vmin=0, vmax=255)\n",
    "\n",
    "# tests\n",
    "env=Env()\n",
    "\n",
    "\n",
    "\n",
    "from mcerl import GridMap\n",
    "test_env_grid_map=GridMap(grid_map)\n",
    "map_to_update=grid_map.copy()\n",
    "map_to_update.fill(127)\n",
    "test_map_tp_update=GridMap(map_to_update)\n",
    "basic_end_points, circle_end_points, circle_end_points_with_polygon, map_to_update,roi_map = (\n",
    "    env._env.test_map_update(\n",
    "        test_env_grid_map, test_map_tp_update, (100, 100), 30, 32, 5\n",
    "    )\n",
    ")\n",
    "a=np.array(map_to_update)\n",
    "b=np.array(roi_map)\n",
    "basic_end_points = [(x / 10000, y / 10000) for x, y in basic_end_points]\n",
    "plt.imshow(b, cmap=\"gray\", vmin=0, vmax=255)\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "plt.imshow(grid_map, cmap=\"gray\", vmin=0, vmax=255)\n",
    "# plt.scatter(*zip(*circle_end_points), c=\"g\", s=2)\n",
    "# plt.scatter(*zip(*basic_end_points), c=\"b\", s=2)\n",
    "plt.scatter(*zip(*circle_end_points_with_polygon), c=\"r\", s=2,marker='x')\n",
    "rect=[(70, 70), (131, 131)]\n",
    "rect=Rectangle((rect[0][0],rect[0][1]),rect[1][0]-rect[0][0],rect[1][1]-rect[0][1],linewidth=1,edgecolor='y',facecolor='none')\n",
    "plt.gca().add_patch(rect)\n",
    "bbx = cv2.boundingRect(np.array(circle_end_points_with_polygon))\n",
    "np.array(circle_end_points_with_polygon) - bbx[0:2]\n",
    "# poly=cv2.fillPoly(b, [np.array(circle_end_points_with_polygon) - bbx[0:2]], 255)\n",
    "# plt.imshow(poly, cmap=\"gray\", vmin=0, vmax=255)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
