{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "from rl.actor_critic import Actor\n",
    "from rl.network import GINNetwork\n",
    "from rl.utils import to_graph\n",
    "\n",
    "from mcerl.env import Env\n",
    "from mcerl.utils import (\n",
    "    delta_time_reward_standardize,\n",
    "    exploration_reward_rescale,\n",
    "    multi_threaded_rollout,\n",
    "    reward_sum,\n",
    "    single_env_rollout,  # noqa: F401\n",
    ")\n",
    "\n",
    "%env SPDLOG_LEVEL=warning\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load map\n",
    "img = Image.open(\"0.png\")\n",
    "img = ImageOps.grayscale(img)\n",
    "img = img.resize((300, 200))\n",
    "grid_map = np.array(img)\n",
    "grid_map[grid_map < 100] = 0\n",
    "grid_map[grid_map >= 100] = 255\n",
    "plt.imshow(grid_map, cmap=\"gray\", vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "from math import pi\n",
    "\n",
    "num_agents = 3\n",
    "agent_poses = None\n",
    "num_rays = 32\n",
    "max_steps = 100000\n",
    "max_steps_per_agent = 100\n",
    "ray_range = 30\n",
    "velocity = 1\n",
    "min_frontier_size = 8\n",
    "max_frontier_size = 30\n",
    "exploration_threshold = 0.98\n",
    "map_height, map_width = grid_map.shape\n",
    "max_step_exploration_reward = ray_range * 2 * 1.41\n",
    "max_exploration_gain = ray_range**2 * pi / 2.0\n",
    "max_step_exploration_reward, max_exploration_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment\n",
    "env = Env(\n",
    "    num_agents=num_agents,\n",
    "    max_steps=max_steps,\n",
    "    max_steps_per_agent=max_steps_per_agent,\n",
    "    velocity=velocity,\n",
    "    sensor_range=ray_range,\n",
    "    num_rays=num_rays,\n",
    "    min_frontier_pixel=min_frontier_size,\n",
    "    max_frontier_pixel=max_frontier_size,\n",
    "    exploration_threshold=exploration_threshold,\n",
    ")\n",
    "# Example usage\n",
    "# num_threads = 15\n",
    "# epochs = 10\n",
    "# rollouts = multi_threaded_rollout(\n",
    "#     env=lambda: Env(\n",
    "#         num_agents=num_agents,\n",
    "#         max_steps=max_steps,\n",
    "#         max_steps_per_agent=max_steps_per_agent,\n",
    "#         velocity=velocity,\n",
    "#         sensor_range=ray_range,\n",
    "#         num_rays=num_rays,\n",
    "#         min_frontier_pixel=min_frontier_size,\n",
    "#         max_frontier_pixel=max_frontier_size,\n",
    "#     ),\n",
    "#     grid_map=grid_map,\n",
    "#     agent_poses=agent_poses,\n",
    "#     policy=policy,\n",
    "#     num_threads=num_threads,\n",
    "#     epochs=epochs,\n",
    "# )\n",
    "# trajectories = []\n",
    "# frame_data = env.reset(grid_map, agent_poses, return_maps=True)\n",
    "# trajectories.append(frame_data)\n",
    "# while True:\n",
    "#     agent_id = frame_data[\"info\"][\"agent_id\"]\n",
    "#     frame_data[\"action_agent_id\"] = agent_id\n",
    "#     frame_data = random_policy(frame_data)\n",
    "#     frame_data = env.step(frame_data, return_maps=True)\n",
    "#     trajectories.append(frame_data)\n",
    "#     if env.done() is True:\n",
    "#         break\n",
    "# rollouts = split_trajectories(trajectories)\n",
    "# rollouts = [pad_trajectory(rollout) for rollout in rollouts]\n",
    "# rollouts = [refine_trajectory(rollout) for rollout in rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define policy\n",
    "\n",
    "\n",
    "from rl.actor_critic import ActorCritic, Critic\n",
    "\n",
    "\n",
    "def forward_preprocess(frame_data: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    normalize position, exploration gain,etc.\n",
    "    \"\"\"\n",
    "    # normalize frontier position to [0,1] and exploration gain to [0,1]\n",
    "    width = float(map_width)\n",
    "    height = float(map_height)\n",
    "    frame_data[\"observation\"][\"frontier_points\"] = [\n",
    "        (\n",
    "            float(x) / width,\n",
    "            float(y) / height,\n",
    "            float(gain) / max_exploration_gain,\n",
    "        )\n",
    "        for x, y, gain in frame_data[\"observation\"][\"frontier_points\"]\n",
    "    ]\n",
    "    # normalize position to [0,1]\n",
    "    frame_data[\"observation\"][\"pos\"] = [\n",
    "        (float(x) / width, float(y) / height)\n",
    "        for x, y in frame_data[\"observation\"][\"pos\"]\n",
    "    ]\n",
    "    frame_data[\"observation\"][\"target_pos\"] = [\n",
    "        (float(x) / width, float(y) / height)\n",
    "        for x, y in frame_data[\"observation\"][\"target_pos\"]\n",
    "    ]\n",
    "    # build graph\n",
    "    frame_data = to_graph(frame_data)\n",
    "    return frame_data\n",
    "\n",
    "\n",
    "policy_network = GINNetwork(dim_h=32)\n",
    "value_network = GINNetwork(dim_h=32)\n",
    "\n",
    "actor = Actor(policy_network=policy_network)\n",
    "critic = Critic(value_network=value_network)\n",
    "wrapped_actor_critic = ActorCritic(\n",
    "    actor=actor, critic=critic, forward_preprocess=forward_preprocess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "# single-threaded rollout\n",
    "\n",
    "single_rollouts = single_env_rollout(env, grid_map, policy=wrapped_actor_critic)\n",
    "rollouts=single_rollouts\n",
    "\n",
    "# multi-threaded rollout\n",
    "# num_threads = 20\n",
    "# epochs = 100\n",
    "# rollouts = multi_threaded_rollout(\n",
    "#     env=lambda: Env(\n",
    "#         num_agents=num_agents,\n",
    "#         max_steps=max_steps,\n",
    "#         max_steps_per_agent=max_steps_per_agent,\n",
    "#         velocity=velocity,\n",
    "#         sensor_range=ray_range,\n",
    "#         num_rays=num_rays,\n",
    "#         min_frontier_pixel=min_frontier_size,\n",
    "#         max_frontier_pixel=max_frontier_size,\n",
    "#     ),\n",
    "#     grid_map=grid_map,\n",
    "#     agent_poses=agent_poses,\n",
    "#     policy=actor,\n",
    "#     num_threads=num_threads,\n",
    "#     epochs=epochs,\n",
    "#     return_maps=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw raw rewards for debugging\n",
    "\n",
    "print(\"exploration reward:\")\n",
    "for rollout in rollouts:\n",
    "    plt.plot(\n",
    "        [frame_data[\"next\"][\"reward\"][\"exploration_reward\"] for frame_data in rollout]\n",
    "    )\n",
    "plt.show()\n",
    "print(\"time_step_reward :\")\n",
    "for rollout in rollouts:\n",
    "    plt.plot(\n",
    "        [frame_data[\"next\"][\"reward\"][\"time_step_reward\"] for frame_data in rollout]\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize rewards\n",
    "rollouts = [\n",
    "    exploration_reward_rescale(rollout, max_value=max_exploration_gain)\n",
    "    for rollout in rollouts\n",
    "]\n",
    "rollouts = [delta_time_reward_standardize(rollout) for rollout in rollouts]\n",
    "rollouts = [reward_sum(rollout) for rollout in rollouts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw normalized rewards for debugging\n",
    "# total_reward\n",
    "for rollout in rollouts:\n",
    "    plt.plot([frame_data[\"next\"][\"reward\"][\"total_reward\"] for frame_data in rollout])\n",
    "mean_val = np.mean(\n",
    "    [\n",
    "        frame_data[\"next\"][\"reward\"][\"total_reward\"]\n",
    "        for frame_data in rollout\n",
    "        for rollout in rollouts\n",
    "    ]\n",
    ")\n",
    "plt.title(f\"total reward:{mean_val}\")\n",
    "plt.axhline(mean_val, color=\"r\", linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# exploration_reward\n",
    "for rollout in rollouts:\n",
    "    plt.plot(\n",
    "        [frame_data[\"next\"][\"reward\"][\"exploration_reward\"] for frame_data in rollout]\n",
    "    )\n",
    "mean_val = np.mean(\n",
    "    [\n",
    "        frame_data[\"next\"][\"reward\"][\"exploration_reward\"]\n",
    "        for frame_data in rollout\n",
    "        for rollout in rollouts\n",
    "    ]\n",
    ")\n",
    "plt.title(f\"exploration reward:{mean_val}\")\n",
    "plt.axhline(mean_val, color=\"r\", linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "# time_step_reward\n",
    "for rollout in rollouts:\n",
    "    plt.plot(\n",
    "        [frame_data[\"next\"][\"reward\"][\"time_step_reward\"] for frame_data in rollout]\n",
    "    )\n",
    "mean_val = np.mean(\n",
    "    [\n",
    "        frame_data[\"next\"][\"reward\"][\"time_step_reward\"]\n",
    "        for frame_data in rollout\n",
    "        for rollout in rollouts\n",
    "    ]\n",
    ")\n",
    "plt.title(f\"time step reward:{mean_val}\")\n",
    "plt.axhline(mean_val, color=\"r\", linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "# reward_to_go\n",
    "for rollout in rollouts:\n",
    "    plt.plot([frame_data[\"reward_to_go\"] for frame_data in rollout])\n",
    "mean_val = np.mean(\n",
    "    [frame_data[\"reward_to_go\"] for frame_data in rollout for rollout in rollouts]\n",
    ")\n",
    "plt.title(f\"reward to go:{mean_val}\")\n",
    "plt.axhline(mean_val, color=\"r\", linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# reward_mean\n",
    "reward_mean = [\n",
    "    np.mean([frame_data[\"reward_to_go\"] for frame_data in rollout])\n",
    "    for rollout in rollouts\n",
    "]\n",
    "plt.plot(reward_mean)\n",
    "plt.title(f\"reward mean:{np.mean(reward_mean)}\")\n",
    "plt.axhline(y=np.mean(reward_mean), color=\"r\", linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store maps to gif\n",
    "# rollouts=single_rollouts\n",
    "# for rollout in rollouts:\n",
    "#     agent_id = rollout[0][\"info\"][\"agent_id\"]\n",
    "#     imgs = [\n",
    "#         Image.fromarray(frame_data[\"observation\"][\"agent_map\"])\n",
    "#         for frame_data in rollout\n",
    "#     ]\n",
    "#     imgs[0].save(\n",
    "#         f\"agent_{agent_id}.gif\",\n",
    "#         save_all=True,\n",
    "#         append_images=imgs[1:],\n",
    "#         duration=100,\n",
    "#         loop=0,\n",
    "#     )\n",
    "# flattened_rollouts = []\n",
    "# for rollout in rollouts:\n",
    "#     flattened_rollouts.extend(rollout)\n",
    "# sorted_rollouts = sorted(flattened_rollouts, key=lambda x: x[\"info\"][\"step_cnt\"])\n",
    "# imgs = [\n",
    "#     Image.fromarray(frame_data[\"observation\"][\"global_map\"])\n",
    "#     for frame_data in sorted_rollouts\n",
    "# ]\n",
    "# imgs[0].save(\n",
    "#     \"global_map.gif\",\n",
    "#     save_all=True,\n",
    "#     append_images=imgs[1:],\n",
    "#     duration=100,\n",
    "#     loop=0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dict_struct(d: dict[str, Any], prefix: str = \"\") -> list:\n",
    "    flattened_keys = []\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            flattened_keys.extend(check_dict_struct(v, prefix + k + \".\"))\n",
    "        else:\n",
    "            flattened_keys.append(prefix + k)\n",
    "    return flattened_keys\n",
    "\n",
    "{len(check_dict_struct(frame_data)) for frame_data in rollout for rollout in rollouts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensordict\n",
    "\n",
    "rollout = rollouts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([35, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        info: TensorDict(\n",
       "            fields={\n",
       "                agent_exploration_rate: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                agent_explored_pixels: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                agent_id: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                agent_step_cnt: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                delta_time: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                global_exploration_rate: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                step_cnt: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "            batch_size=torch.Size([35]),\n",
       "            device=None,\n",
       "            is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                info: TensorDict(\n",
       "                    fields={\n",
       "                        agent_exploration_rate: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        agent_explored_pixels: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                        agent_id: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                        agent_step_cnt: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                        delta_time: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                        global_exploration_rate: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        step_cnt: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
       "                    batch_size=torch.Size([35]),\n",
       "                    device=None,\n",
       "                    is_shared=False),\n",
       "                observation: LazyStackedTensorDict(\n",
       "                    fields={\n",
       "                        agent_map: Tensor(shape=torch.Size([35, 200, 300]), device=cpu, dtype=torch.uint8, is_shared=False),\n",
       "                        batch_index: Tensor(shape=torch.Size([35, -1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                        edge_index: Tensor(shape=torch.Size([35, 2, -1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                        frontier_points: Tensor(shape=torch.Size([35, -1, 3]), device=cpu, dtype=torch.float64, is_shared=False),\n",
       "                        global_map: Tensor(shape=torch.Size([35, 200, 300]), device=cpu, dtype=torch.uint8, is_shared=False),\n",
       "                        graph: NonTensorStack(\n",
       "                            [DataBatch(x=[18, 5], edge_index=[2, 24], batch=[1...,\n",
       "                            batch_size=torch.Size([35]),\n",
       "                            device=None),\n",
       "                        pos: Tensor(shape=torch.Size([35, 3, 2]), device=cpu, dtype=torch.float64, is_shared=False),\n",
       "                        target_pos: Tensor(shape=torch.Size([35, 3, 2]), device=cpu, dtype=torch.float64, is_shared=False),\n",
       "                        x: Tensor(shape=torch.Size([35, -1, 5]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                    exclusive_fields={\n",
       "                    },\n",
       "                    batch_size=torch.Size([35]),\n",
       "                    device=None,\n",
       "                    is_shared=False,\n",
       "                    stack_dim=0),\n",
       "                reward: TensorDict(\n",
       "                    fields={\n",
       "                        exploration_reward: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        time_step_reward: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        total_reward: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                    batch_size=torch.Size([35]),\n",
       "                    device=None,\n",
       "                    is_shared=False)},\n",
       "            batch_size=torch.Size([35]),\n",
       "            device=None,\n",
       "            is_shared=False),\n",
       "        observation: LazyStackedTensorDict(\n",
       "            fields={\n",
       "                agent_map: Tensor(shape=torch.Size([35, 200, 300]), device=cpu, dtype=torch.uint8, is_shared=False),\n",
       "                batch_index: Tensor(shape=torch.Size([35, -1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                edge_index: Tensor(shape=torch.Size([35, 2, -1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "                frontier_points: Tensor(shape=torch.Size([35, -1, 3]), device=cpu, dtype=torch.float64, is_shared=False),\n",
       "                global_map: Tensor(shape=torch.Size([35, 200, 300]), device=cpu, dtype=torch.uint8, is_shared=False),\n",
       "                graph: NonTensorStack(\n",
       "                    [DataBatch(x=[15, 5], edge_index=[2, 20], batch=[1...,\n",
       "                    batch_size=torch.Size([35]),\n",
       "                    device=None),\n",
       "                pos: Tensor(shape=torch.Size([35, 3, 2]), device=cpu, dtype=torch.float64, is_shared=False),\n",
       "                target_pos: Tensor(shape=torch.Size([35, 3, 2]), device=cpu, dtype=torch.float64, is_shared=False),\n",
       "                x: Tensor(shape=torch.Size([35, -1, 5]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "            exclusive_fields={\n",
       "            },\n",
       "            batch_size=torch.Size([35]),\n",
       "            device=None,\n",
       "            is_shared=False,\n",
       "            stack_dim=0),\n",
       "        reward_to_go: Tensor(shape=torch.Size([35]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "    batch_size=torch.Size([35]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "for i in range(len(rollout)):\n",
    "    graph = rollout[i][\"observation\"][\"graph\"]\n",
    "    rollout[i][\"observation\"].update(\n",
    "        {\"x\": graph.x, \"edge_index\": graph.edge_index, \"batch_index\": graph.batch}\n",
    "    )\n",
    "    rollout[i][\"next\"][\"observation\"].update(\n",
    "        {\"x\": graph.x, \"edge_index\": graph.edge_index, \"batch_index\": graph.batch}\n",
    "    )\n",
    "tds = [tensordict.TensorDict.from_dict(frame_data) for frame_data in rollout]\n",
    "tds = tensordict.LazyStackedTensorDict.maybe_dense_stack(tds)\n",
    "tds[\"next\", \"reward\", \"total_reward\"] = torch.zeros_like(\n",
    "    tds[\"next\", \"reward\", \"total_reward\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
